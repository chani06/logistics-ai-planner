import streamlit as st
import pandas as pd
import numpy as np
import io
import os
import glob
import networkx as nx
from sklearn.cluster import DBSCAN
import math
import warnings

warnings.filterwarnings('ignore')

# ==========================================
# 1. CONFIG
# ==========================================
LIMITS = {'4W': {'max_w': 2500, 'max_c': 5.0}, 'JB': {'max_w': 3500, 'max_c': 8.0}, '6W': {'max_w': 5800, 'max_c': 22.0}}
BUFFER = 1.05
MAX_KM_CLUSTER = 30.0
TARGET_DROPS = 10
MAX_DROPS_FLEX = 12
NEARBY_RADIUS = 5.0
MAX_ZONE_DISTANCE = 100.0
STRICT_ZONE_MODE = False
HISTORICAL_ONLY_MODE = True  # ‡∏à‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥

# Utilization thresholds for truck optimization
MIN_CUBE_UTILIZATION = 0.90  # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≥ 90% ‡∏Å‡πà‡∏≠‡∏ô‡∏õ‡∏¥‡∏î‡∏£‡∏ñ
TARGET_CUBE_UTILIZATION = 1.00  # ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ 100%
FLEX_CUBE_UTILIZATION = 1.05  # ‡∏¢‡∏≠‡∏°‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏î‡πâ‡∏ñ‡∏∂‡∏á 105%

EXCLUDE = ['PTDC', 'Distribution Center', 'DC‡∏ß‡∏±‡∏á‡∏ô‡πâ‡∏≠‡∏¢', 'DC011']

# ==========================================
# 2. HELPER FUNCTIONS
# ==========================================
def normalize(val):
    return str(val).strip().upper().replace(" ", "").replace(".0", "")

def haversine(lat1, lon1, lat2, lon2):
    R = 6371
    dLat = math.radians(lat2 - lat1)
    dLon = math.radians(lon2 - lon1)
    a = math.sin(dLat/2) * math.sin(dLat/2) + \
        math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * \
        math.sin(dLon/2) * math.sin(dLon/2)
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
    return R * c

def is_similar_name(name1, name2):
    def clean(n):
        return ''.join([c for c in str(n) if c.isalpha()])
    return clean(name1) == clean(name2) and len(clean(name1)) > 3

def get_province_zone(province):
    if not province or pd.isna(province):
        return 'UNKNOWN'
    
    prov = str(province).strip()
    
    central = ['‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û', '‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ', '‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ', '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£', '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏≤‡∏Ñ‡∏£', '‡∏ô‡∏Ñ‡∏£‡∏õ‡∏ê‡∏°', 
               '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°', '‡∏£‡∏≤‡∏ä‡∏ö‡∏∏‡∏£‡∏µ', '‡∏Å‡∏≤‡∏ç‡∏à‡∏ô‡∏ö‡∏∏‡∏£‡∏µ', '‡∏™‡∏∏‡∏û‡∏£‡∏£‡∏ì‡∏ö‡∏∏‡∏£‡∏µ', '‡∏ä‡∏±‡∏¢‡∏ô‡∏≤‡∏ó', '‡∏™‡∏¥‡∏á‡∏´‡πå‡∏ö‡∏∏‡∏£‡∏µ', 
               '‡∏≠‡πà‡∏≤‡∏á‡∏ó‡∏≠‡∏á', '‡∏•‡∏û‡∏ö‡∏∏‡∏£‡∏µ', '‡∏™‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏µ', '‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤', '‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤']
    
    northeast = ['‡∏ô‡∏Ñ‡∏£‡∏£‡∏≤‡∏ä‡∏™‡∏µ‡∏°‡∏≤', '‡πÇ‡∏Ñ‡∏£‡∏≤‡∏ä', '‡∏ö‡∏∏‡∏£‡∏µ‡∏£‡∏±‡∏°‡∏¢‡πå', '‡∏™‡∏∏‡∏£‡∏¥‡∏ô‡∏ó‡∏£‡πå', '‡∏®‡∏µ‡∏Ç‡∏£‡∏†‡∏π‡∏°‡∏¥', '‡∏Ç‡∏≠‡∏ô‡πÅ‡∏Å‡πà‡∏ô', 
                 '‡∏≠‡∏∏‡∏î‡∏£‡∏ò‡∏≤‡∏ô‡∏µ', '‡πÄ‡∏•‡∏¢', '‡∏´‡∏ô‡∏≠‡∏á‡∏Ñ‡∏≤‡∏¢', '‡∏°‡∏´‡∏≤‡∏™‡∏≤‡∏£‡∏Ñ‡∏≤‡∏°', '‡∏£‡πâ‡∏≠‡∏¢‡πÄ‡∏≠‡πá‡∏î', '‡∏Å‡∏≤‡∏¨‡∏™‡∏¥‡∏ô‡∏ò‡∏∏‡πå', 
                 '‡∏™‡∏Å‡∏•‡∏ô‡∏Ñ‡∏£', '‡∏ô‡∏Ñ‡∏£‡∏û‡∏ô‡∏°', '‡∏°‡∏∏‡∏Å‡∏î‡∏≤‡∏´‡∏≤‡∏£', '‡∏¢‡πÇ‡∏™‡∏ò‡∏£', '‡∏≠‡∏≥‡∏ô‡∏≤‡∏à‡πÄ‡∏à‡∏£‡∏¥‡∏ç', '‡∏≠‡∏∏‡∏ö‡∏•‡∏£‡∏≤‡∏ä‡∏ò‡∏≤‡∏ô‡∏µ', 
                 '‡∏ä‡∏±‡∏¢‡∏†‡∏π‡∏°‡∏¥', '‡∏ö‡∏∂‡∏á‡∏Å‡∏≤‡∏¨']
    
    north = ['‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà', '‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡∏£‡∏≤‡∏¢', '‡∏•‡∏≥‡∏û‡∏π‡∏ô', '‡∏•‡∏≥‡∏õ‡∏≤‡∏á', '‡∏û‡∏∞‡πÄ‡∏¢‡∏≤', '‡πÅ‡∏û‡∏£‡πà', '‡∏ô‡πà‡∏≤‡∏ô', 
             '‡∏≠‡∏∏‡∏ï‡∏£‡∏î‡∏¥‡∏ï‡∏ñ‡πå', '‡∏ï‡∏≤‡∏Å', '‡∏™‡∏∏‡πÇ‡∏Ç‡∏ó‡∏±‡∏¢', '‡∏û‡∏¥‡∏©‡∏ì‡∏∏‡πÇ‡∏•‡∏Å', '‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£', '‡πÄ‡∏û‡∏ä‡∏£‡∏ö‡∏π‡∏£‡∏ì‡πå', '‡∏Å‡∏≥‡πÅ‡∏û‡∏á‡πÄ‡∏û‡∏ä‡∏£']
    
    south = ['‡∏ä‡∏∏‡∏°‡∏û‡∏£', '‡∏™‡∏∏‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ò‡∏≤‡∏ô‡∏µ', '‡∏£‡∏∞‡∏ô‡∏≠‡∏á', '‡∏û‡∏±‡∏á‡∏á‡∏≤', '‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï', '‡∏Å‡∏£‡∏∞‡∏ö‡∏µ‡πà', '‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏ò‡∏£‡∏£‡∏°‡∏£‡∏≤‡∏ä', 
             '‡∏ï‡∏£‡∏±‡∏á', '‡∏û‡∏±‡∏ó‡∏•‡∏∏‡∏á', '‡∏™‡∏á‡∏Ç‡∏•‡∏≤', '‡∏™‡∏ï‡∏π‡∏•', '‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ', '‡∏¢‡∏∞‡∏•‡∏≤', '‡∏ô‡∏£‡∏≤‡∏ò‡∏¥‡∏ß‡∏≤‡∏™']
    
    east = ['‡∏â‡∏∞‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏ó‡∏£‡∏≤', '‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ', '‡∏£‡∏∞‡∏¢‡∏≠‡∏á', '‡∏à‡∏±‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ', '‡∏ï‡∏£‡∏≤‡∏î', '‡∏õ‡∏£‡∏≤‡∏à‡∏µ‡∏ô‡∏ö‡∏∏‡∏£‡∏µ', '‡∏™‡∏£‡∏∞‡πÅ‡∏Å‡πâ‡∏ß']
    
    west = ['‡∏Å‡∏≤‡∏ç‡∏à‡∏ô‡∏ö‡∏∏‡∏£‡∏µ', '‡∏ï‡∏≤‡∏Å', '‡∏õ‡∏£‡∏∞‡∏à‡∏ß‡∏ö‡∏Ñ‡∏µ‡∏£‡∏µ‡∏Ç‡∏±‡∏ô‡∏ò‡πå', '‡πÄ‡∏û‡∏ä‡∏£‡∏ö‡∏∏‡∏£‡∏µ']
    
    for p in central:
        if p in prov: return 'CENTRAL'
    for p in northeast:
        if p in prov: return 'NORTHEAST'
    for p in north:
        if p in prov: return 'NORTH'
    for p in south:
        if p in prov: return 'SOUTH'
    for p in east:
        if p in prov: return 'EAST'
    for p in west:
        if p in prov: return 'WEST'
    
    return 'UNKNOWN'

def is_same_zone(code1, code2, zone_map, geo):
    """‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ 2 ‡∏™‡∏≤‡∏Ç‡∏≤‡∏≠‡∏¢‡∏π‡πà zone ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà - ‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î‡∏°‡∏≤‡∏Å"""
    if not STRICT_ZONE_MODE:
        return True
    
    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ‡∏Å‡πà‡∏≠‡∏ô - ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô 100%
    zone1 = zone_map.get(code1, 'UNKNOWN')
    zone2 = zone_map.get(code2, 'UNKNOWN')
    
    # ‡∏ñ‡πâ‡∏≤‡∏£‡∏π‡πâ‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ‡∏ó‡∏±‡πâ‡∏á 2 ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô ‡πÑ‡∏°‡πà‡∏á‡∏±‡πâ‡∏ô‡∏´‡πâ‡∏≤‡∏°‡∏£‡∏ß‡∏°
    if zone1 != 'UNKNOWN' and zone2 != 'UNKNOWN':
        if zone1 != zone2:
            return False
    
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á ‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á
    if code1 in geo and code2 in geo:
        lat1, lon1 = geo[code1]
        lat2, lon2 = geo[code2]
        if lat1 != 0 and lat2 != 0:
            dist = haversine(lat1, lon1, lat2, lon2)
            if dist > MAX_ZONE_DISTANCE:
                return False
    else:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏û‡∏¥‡∏Å‡∏±‡∏î‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ ‡∏´‡πâ‡∏≤‡∏°‡∏£‡∏ß‡∏°
        if zone1 == 'UNKNOWN' or zone2 == 'UNKNOWN':
            return False
    
    return True

# ==========================================
# 3. LOADERS & PROCESSORS
# ==========================================
def load_excel(content, sheet_name=None):
    try:
        xls = pd.ExcelFile(io.BytesIO(content))
        target_sheet = None
        
        # ‡∏ñ‡πâ‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏µ‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞
        if sheet_name:
            if sheet_name in xls.sheet_names:
                target_sheet = sheet_name
            else:
                # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏ä‡∏µ‡∏ï‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô
                for s in xls.sheet_names:
                    if sheet_name.lower() in s.lower():
                        target_sheet = s
                        break
        
        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏ä‡πâ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        if not target_sheet:
            priority = ['2.punthai', '2.', 'punthai', 'order', 'history', 'data', 'sheet']
            
            for p in priority:
                for s in xls.sheet_names:
                    if p in s.lower(): 
                        target_sheet = s
                        break
                if target_sheet: break
        
        if not target_sheet: target_sheet = xls.sheet_names[0]
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ header row ‡πÇ‡∏î‡∏¢‡∏î‡∏π‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î
        df_tmp = pd.read_excel(xls, sheet_name=target_sheet, nrows=30, header=None)
        h_row = -1
        
        keywords = ['CODE', 'BRANCH', '‡∏™‡∏≤‡∏Ç‡∏≤', 'WGT', 'CUBE', '‡∏Ñ‡∏¥‡∏ß', '‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å', 
                   'TRIP', 'BOOKING', '‡∏£‡∏´‡∏±‡∏™', '‡∏ó‡∏£‡∏¥‡∏õ', 'LAT', 'LON', 'VEHICLE']
        
        for i, r in df_tmp.iterrows():
            row_str = r.astype(str).str.upper().tolist()
            # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ß
            match_count = sum(1 for k in keywords if any(k in s for s in row_str))
            if match_count >= 3:  # ‡∏ñ‡πâ‡∏≤‡∏û‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3 ‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î = header
                h_row = i
                break
        
        if h_row == -1: h_row = 0  # ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏ä‡πâ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å
        
        df = pd.read_excel(xls, sheet_name=target_sheet, header=h_row)
        return df
    except Exception as e:
        st.error(f"‚ùå Error loading Excel sheet '{sheet_name}': {str(e)}")
        return None

def process_dataframe(df):
    if df is None: return None
    df.columns = df.columns.astype(str).str.strip()
    df = df.loc[:, ~df.columns.duplicated()]
    rename_map = {}
    for c in df.columns:
        c_stripped = c.strip()
        cu = c.upper().replace(' ','').replace('_','')
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏ö‡∏ö‡∏ï‡∏£‡∏á‡∏ï‡∏±‡∏ß (exact match) ‡∏Å‡πà‡∏≠‡∏ô
        if c_stripped == 'BranchCode' or c_stripped == '‡∏£‡∏´‡∏±‡∏™ WMS':
            rename_map[c] = 'Code'
        elif c_stripped == 'Branch':
            rename_map[c] = 'Name'
        elif c_stripped == 'TOTALWGT':
            rename_map[c] = 'Wgt'
        elif c_stripped == 'TOTALCUBE':
            rename_map[c] = 'Cube'
        elif c_stripped == 'latitude' or c_stripped == ' latitude ':
            rename_map[c] = 'Lat'
        elif c_stripped == 'longitude':
            rename_map[c] = 'Lon'
        elif c_stripped == 'Trip':
            rename_map[c] = 'Trip'
        elif c_stripped == 'Trip no':
            rename_map[c] = 'Vehicle'
        elif c_stripped == '‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î':
            rename_map[c] = 'Province'
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡πÅ‡∏ö‡∏ö exact ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ partial match
        elif 'BRANCHCODE' in cu or '‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤' in cu:
            rename_map[c] = 'Code'
        elif 'WGT' in cu or '‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å' in cu:
            rename_map[c] = 'Wgt'
        elif 'CUBE' in cu or '‡∏Ñ‡∏¥‡∏ß' in cu:
            rename_map[c] = 'Cube'
        elif 'LAT' in cu:
            rename_map[c] = 'Lat'
        elif 'LON' in cu:
            rename_map[c] = 'Lon'
        elif 'BOOKING' in cu:
            rename_map[c] = 'Trip'
        elif 'VEHICLE' in cu or 'TRIPNO' in cu:
            rename_map[c] = 'Vehicle'
    
    df.rename(columns=rename_map, inplace=True)
    
    # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï index ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô duplicate labels
    df = df.reset_index(drop=True)
    
    if 'Code' not in df.columns:
        if 'Name' in df.columns: df['Code'] = df['Name']
        else: return None
        
    df['Code'] = df['Code'].apply(normalize)
    for c in ['Wgt','Cube','Lat','Lon']:
        if c not in df.columns: df[c] = 0.0
        else: df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0.0)
    
    # ‡πÉ‡∏ä‡πâ numpy array ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤ duplicate index
    import numpy as np
    mask_to_keep = ~df['Code'].isin(EXCLUDE).values
    
    if 'Name' in df.columns:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á mask ‡∏à‡∏≤‡∏Å Name ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ numpy array
        name_str = df['Name'].astype(str).values
        for exclude_key in EXCLUDE:
            name_mask = np.array([exclude_key not in s for s in name_str])
            mask_to_keep = mask_to_keep & name_mask
    
    # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ boolean indexing
    df = df[mask_to_keep].reset_index(drop=True)
    
    return df.copy()

def process_geo(df):
    if df is None: return {}
    # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á process_dataframe ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ df ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏ñ‡∏π‡∏Å process ‡πÅ‡∏•‡πâ‡∏ß
    geo = {}
    if df is not None and 'Code' in df.columns and 'Lat' in df.columns and 'Lon' in df.columns:
        for _, r in df.iterrows():
            if pd.notna(r['Lat']) and r['Lat'] != 0 and pd.notna(r['Code']):
                code = normalize(str(r['Code']))
                geo[code] = (float(r['Lat']), float(r['Lon']))
    return geo

# ==========================================
# 4. AI CORE
# ==========================================
def train_ai(df_list):
    G = nx.Graph()
    req = {}
    zones = {}
    regions = {}
    trip_distances = {}  # ‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ó‡∏£‡∏¥‡∏õ
    trip_patterns = []   # ‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î
    
    for df in df_list:
        if df is None or 'Trip' not in df.columns: continue
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á copy ‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ã‡πâ‡∏≥
        df = df.copy()
        df = df.loc[:, ~df.columns.duplicated()]
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Trip ‡πÄ‡∏õ‡πá‡∏ô DataFrame
        if isinstance(df['Trip'], pd.DataFrame):
            df['Trip'] = df['Trip'].iloc[:,0]
        
        # ‡πÅ‡∏õ‡∏•‡∏á Trip ‡πÄ‡∏õ‡πá‡∏ô string ‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        df['Trip'] = df['Trip'].astype(str)
        df = df[(df['Trip'].notna()) & (df['Trip'] != 'nan') & (df['Trip'] != '') & (df['Trip'] != 'None')]
        
        if len(df) == 0:
            continue
        
        # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡πÅ‡∏•‡∏∞‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ
        for idx, r in df.iterrows():
            if 'Province' in df.columns and pd.notna(r['Province']):
                prov = str(r['Province']).strip()
                zones[r['Code']] = prov
                regions[r['Code']] = get_province_zone(prov)
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏£‡∏¥‡∏õ
        for t, g in df.groupby('Trip'):
            codes = g['Code'].unique()
            veh = str(g['Vehicle'].iloc[0]).upper() if 'Vehicle' in g.columns else ''
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏£‡∏ñ‡∏à‡∏≤‡∏Å Trip no (4W, 6W, JB)
            rank = 3 if '6W' in veh or '6‡∏•‡πâ‡∏≠' in veh else (2 if 'JB' in veh or '‡∏à‡∏±‡∏°‡πÇ‡∏ö' in veh else 1)
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å requirement ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡∏≤‡∏Ç‡∏≤
            for c in codes: 
                req[c] = max(req.get(c,1), rank)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏ó‡∏£‡∏¥‡∏õ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏û‡∏¥‡∏Å‡∏±‡∏î)
            if 'Lat' in g.columns and 'Lon' in g.columns:
                total_dist = 0
                coords = g[['Lat', 'Lon']].values
                for i in range(len(coords)-1):
                    if coords[i][0] != 0 and coords[i+1][0] != 0:
                        total_dist += haversine(coords[i][0], coords[i][1], 
                                               coords[i+1][0], coords[i+1][1])
                
                if total_dist > 0:
                    trip_distances[t] = total_dist
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏£‡∏¥‡∏õ
            trip_info = {
                'trip': t,
                'branches': len(codes),
                'vehicle': veh,
                'weight': g['Wgt'].sum() if 'Wgt' in g.columns else 0,
                'cube': g['Cube'].sum() if 'Cube' in g.columns else 0,
                'codes': list(codes),
                'region': regions.get(codes[0], 'UNKNOWN') if len(codes) > 0 else 'UNKNOWN'
            }
            trip_patterns.append(trip_info)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå (‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô)
            if len(codes)>1:
                for i in range(len(codes)):
                    for j in range(i+1, len(codes)): 
                        G.add_edge(codes[i], codes[j])
            elif len(codes)==1: 
                G.add_node(codes[0])
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
    learning_stats = {
        'total_trips': len(trip_patterns),
        'total_branches': len(req),
        'avg_drops': sum(p['branches'] for p in trip_patterns) / len(trip_patterns) if trip_patterns else 0,
        'avg_distance': sum(trip_distances.values()) / len(trip_distances) if trip_distances else 0,
        'region_distribution': {},
        'vehicle_usage': {}
    }
    
    # ‡∏ô‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ
    for pattern in trip_patterns:
        region = pattern['region']
        learning_stats['region_distribution'][region] = learning_stats['region_distribution'].get(region, 0) + 1
        
        veh = pattern['vehicle']
        if '6' in veh:
            veh_type = '6W'
        elif 'J' in veh or '‡∏à‡∏±‡∏°‡πÇ‡∏ö' in veh:
            veh_type = '4W-JB'
        else:
            veh_type = '4W'
        learning_stats['vehicle_usage'][veh_type] = learning_stats['vehicle_usage'].get(veh_type, 0) + 1
    
    return G, req, regions, learning_stats

def select_truck(w, c, min_rank, avg_distance=0, cube_utilization=0):
    """
    ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏ñ‡∏ï‡∏≤‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å ‡∏Ñ‡∏¥‡∏ß ‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
    
    ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå:
    1. ‡πÉ‡∏ä‡πâ 4W ‡∏ñ‡πâ‡∏≤‡πÉ‡∏™‡πà‡πÑ‡∏î‡πâ‡∏û‡∏≠‡∏î‡∏µ
    2. ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô 4W ‚Üí ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏ä‡πâ 4W Jumbo ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤ (>90% cube)
    3. ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ 6W ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ utilization ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å (>90%)
    4. ‡∏ñ‡πâ‡∏≤‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡πÑ‡∏Å‡∏•‡∏°‡∏≤‡∏Å (>150km) ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ 6W ‚Üí ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏ñ‡∏∂‡∏á 80% cube
    """
    
    # ‡∏ñ‡πâ‡∏≤ requirement ‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ 6W
    if min_rank >= 3:
        return '6 ‡∏•‡πâ‡∏≠ ‡∏ï‡∏π‡πâ‡∏ó‡∏∂‡∏ö'
    
    # 4W ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤: ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å ‚â§ 2500 kg ‡πÅ‡∏•‡∏∞ ‡∏Ñ‡∏¥‡∏ß ‚â§ 5.0
    if w <= LIMITS['4W']['max_w'] and c <= LIMITS['4W']['max_c']:
        return '4 ‡∏•‡πâ‡∏≠ ‡∏ï‡∏π‡πâ‡∏ó‡∏∂‡∏ö'
    
    # 4W ‡∏à‡∏±‡∏°‡πÇ‡∏ö‡πâ: ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å ‚â§ 3500 kg ‡πÅ‡∏•‡∏∞ ‡∏Ñ‡∏¥‡∏ß ‚â§ 8.0
    if w <= LIMITS['JB']['max_w'] and c <= LIMITS['JB']['max_c']:
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì utilization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 4W Jumbo
        jumbo_util = c / LIMITS['JB']['max_c']
        
        # ‡∏ñ‡πâ‡∏≤ utilization ‡∏î‡∏µ (>70%) ‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏ï‡πá‡∏° ‚Üí ‡πÉ‡∏ä‡πâ 4W Jumbo
        if jumbo_util >= 0.70 or w >= LIMITS['JB']['max_w'] * 0.80:
            return '4 ‡∏•‡πâ‡∏≠ ‡∏à‡∏±‡∏°‡πÇ‡∏ö‡πâ ‡∏ï‡∏π‡πâ‡∏ó‡∏∂‡∏ö'
        
        # ‡∏ñ‡πâ‡∏≤ utilization ‡∏ï‡πà‡∏≥ ‡πÅ‡∏ï‡πà‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏Å‡∏• (<100km) ‚Üí ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ 4W Jumbo ‡πÑ‡∏î‡πâ
        if avg_distance < 100:
            return '4 ‡∏•‡πâ‡∏≠ ‡∏à‡∏±‡∏°‡πÇ‡∏ö‡πâ ‡∏ï‡∏π‡πâ‡∏ó‡∏∂‡∏ö'
    
    # ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ 6W
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì utilization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 6W
    six_wheel_util = c / LIMITS['6W']['max_c']
    
    # ‡∏ñ‡πâ‡∏≤‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡πÑ‡∏Å‡∏•‡∏°‡∏≤‡∏Å (>150km) ‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ 6W ‚Üí ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö utilization ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤
    if avg_distance > 150:
        if six_wheel_util >= 0.80:  # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 80% ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏Å‡∏•
            return '6 ‡∏•‡πâ‡∏≠ ‡∏ï‡∏π‡πâ‡∏ó‡∏∂‡∏ö'
    
    # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ: 6W ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ utilization ‡∏™‡∏π‡∏á (>90%)
    if six_wheel_util >= MIN_CUBE_UTILIZATION:
        return '6 ‡∏•‡πâ‡∏≠ ‡∏ï‡∏π‡πâ‡∏ó‡∏∂‡∏ö'
    
    # ‡∏ñ‡πâ‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô 4W Jumbo ‡πÅ‡∏ï‡πà cube ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏ï‡πá‡∏° ‚Üí ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ 6W ‡πÅ‡∏ï‡πà‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô
    if w > LIMITS['JB']['max_w']:
        return '6 ‡∏•‡πâ‡∏≠ ‡∏ï‡∏π‡πâ‡∏ó‡∏∂‡∏ö'
    
    # Default: 4W Jumbo
    return '4 ‡∏•‡πâ‡∏≠ ‡∏à‡∏±‡∏°‡πÇ‡∏ö‡πâ ‡∏ï‡∏π‡πâ‡∏ó‡∏∂‡∏ö'

def merge_small_trips(df_result, geo, region_map):
    """‡∏£‡∏ß‡∏°‡∏ó‡∏£‡∏¥‡∏õ‡πÄ‡∏•‡πá‡∏Å‡πÜ (1-3 ‡∏à‡∏∏‡∏î) ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô ‡πÅ‡∏ö‡∏ö‡∏Å‡πâ‡∏≤‡∏ß‡∏£‡πâ‡∏≤‡∏ß"""
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ó‡∏£‡∏¥‡∏õ
    trip_stats = df_result.groupby('Booking No').agg({
        '‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤': 'count',
        'TOTALWGT': 'sum',
        'TOTALCUBE': 'sum'
    }).rename(columns={'‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤': 'drops'})
    
    # ‡∏´‡∏≤‡∏ó‡∏£‡∏¥‡∏õ‡πÄ‡∏•‡πá‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ (‚â§ 3 ‡∏à‡∏∏‡∏î, ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å < 2000 kg, ‡∏Ñ‡∏¥‡∏ß < 5.0)
    small_trips = trip_stats[(trip_stats['drops'] <= 3) & 
                            (trip_stats['TOTALWGT'] < 2000) & 
                            (trip_stats['TOTALCUBE'] < 5.0)].index.tolist()
    
    if not small_trips:
        return df_result
    
    # ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏£‡∏¥‡∏õ‡πÄ‡∏•‡πá‡∏Å‡∏ï‡∏≤‡∏°‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ
    trip_by_region = {}
    for trip_id in small_trips:
        trip_data = df_result[df_result['Booking No'] == trip_id]
        first_code = trip_data.iloc[0]['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤']
        region = region_map.get(first_code, 'UNKNOWN')
        
        if region not in trip_by_region:
            trip_by_region[region] = []
        
        # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏£‡∏¥‡∏õ
        trip_info = {
            'trip_id': trip_id,
            'data': trip_data,
            'weight': trip_stats.loc[trip_id, 'TOTALWGT'],
            'cube': trip_stats.loc[trip_id, 'TOTALCUBE'],
            'drops': trip_stats.loc[trip_id, 'drops'],
            'codes': trip_data['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤'].tolist()
        }
        trip_by_region[region].append(trip_info)
    
    # ‡∏£‡∏ß‡∏°‡∏ó‡∏£‡∏¥‡∏õ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ
    new_rows = []
    merged_trips = set()
    
    for region, trips in trip_by_region.items():
        if len(trips) <= 1 or region == 'UNKNOWN':
            continue
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢
        trips.sort(key=lambda x: x['cube'], reverse=True)
        
        i = 0
        while i < len(trips):
            if trips[i]['trip_id'] in merged_trips:
                i += 1
                continue
            
            # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏ñ‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏ó‡∏£‡∏¥‡∏õ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            current_group = [trips[i]]
            merged_trips.add(trips[i]['trip_id'])
            
            curr_w = trips[i]['weight']
            curr_c = trips[i]['cube']
            curr_drops = trips[i]['drops']
            
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏£‡∏ß‡∏°‡∏ó‡∏£‡∏¥‡∏õ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
            j = i + 1
            while j < len(trips):
                if trips[j]['trip_id'] in merged_trips:
                    j += 1
                    continue
                
                new_w = curr_w + trips[j]['weight']
                new_c = curr_c + trips[j]['cube']
                new_drops = curr_drops + trips[j]['drops']
                
                # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏° (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 4W Jumbo ‡∏´‡∏£‡∏∑‡∏≠ 12 ‡∏à‡∏∏‡∏î)
                if new_w <= 3500 and new_c <= 8.0 and new_drops <= MAX_DROPS_FLEX:
                    current_group.append(trips[j])
                    merged_trips.add(trips[j]['trip_id'])
                    curr_w = new_w
                    curr_c = new_c
                    curr_drops = new_drops
                elif new_w <= 5800 and new_c <= 22.0 and new_drops <= MAX_DROPS_FLEX:
                    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô 4W Jumbo ‡πÅ‡∏ï‡πà‡πÉ‡∏™‡πà 6W ‡πÑ‡∏î‡πâ
                    current_group.append(trips[j])
                    merged_trips.add(trips[j]['trip_id'])
                    curr_w = new_w
                    curr_c = new_c
                    curr_drops = new_drops
                
                j += 1
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ô‡∏µ‡πâ
            if len(current_group) > 1:
                # ‡∏£‡∏ß‡∏°‡∏ó‡∏£‡∏¥‡∏õ
                for trip_info in current_group:
                    for _, row in trip_info['data'].iterrows():
                        row_dict = row.to_dict()
                        row_dict['Booking No'] = f"MERGED-{region}-{len(new_rows)}"
                        row_dict['Remark'] = f"Drops:{curr_drops}"
                        new_rows.append(row_dict)
            else:
                # ‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° - ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏£‡∏¥‡∏õ‡πÄ‡∏î‡∏¥‡∏°
                for _, row in current_group[0]['data'].iterrows():
                    new_rows.append(row.to_dict())
            
            i += 1
    
    # ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏£‡∏¥‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏ß‡∏°
    for _, row in df_result.iterrows():
        if row['Booking No'] not in merged_trips:
            new_rows.append(row.to_dict())
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÉ‡∏´‡∏°‡πà
    if new_rows:
        df_merged = pd.DataFrame(new_rows)
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö Booking No ‡πÉ‡∏´‡∏°‡πà
        unique_bookings = sorted(df_merged['Booking No'].unique())
        booking_map = {old: f"AI-{i+1:03d}" for i, old in enumerate(unique_bookings)}
        df_merged['Booking No'] = df_merged['Booking No'].map(booking_map)
        
        return df_merged
    
    return df_result

def run_prediction(df_test, G, geo, constraints, region_map):
    df_test['Lat'] = df_test.apply(lambda r: geo.get(r['Code'],(0,0))[0] if r['Lat']==0 else r['Lat'], axis=1)
    df_test['Lon'] = df_test.apply(lambda r: geo.get(r['Code'],(0,0))[1] if r['Lon']==0 else r['Lon'], axis=1)
    df_test['Region'] = df_test['Code'].map(lambda x: region_map.get(x, 'UNKNOWN'))
    
    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î HISTORICAL_ONLY_MODE ‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡∏≤‡∏Ç‡∏≤‡∏°‡∏µ Cluster ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á (‡πÅ‡∏¢‡∏Å‡∏ó‡∏£‡∏¥‡∏õ)
    # ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏£‡∏ß‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏£‡∏¥‡∏õ
    if HISTORICAL_ONLY_MODE:
        # ‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡∏≤‡∏Ç‡∏≤‡∏°‡∏µ cluster ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡∏•‡∏∞‡∏™‡∏≤‡∏Ç‡∏≤
        df_test['Cluster'] = df_test['Code'].map(lambda x: f"SINGLE-{x}")
    else:
        # ‡πÇ‡∏´‡∏°‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ - ‡πÉ‡∏ä‡πâ connected components
        hist_map = {n:i for i,c in enumerate(nx.connected_components(G)) for n in c}
        df_test['Cluster'] = df_test['Code'].map(lambda x: f"H-{hist_map[x]}" if x in hist_map else "UNK")
        
        if STRICT_ZONE_MODE:
            new_clusters = []
            for idx, row in df_test.iterrows():
                if row['Cluster'] != 'UNK' and row['Region'] != 'UNKNOWN':
                    new_clusters.append(f"{row['Cluster']}-{row['Region']}")
                else:
                    new_clusters.append(row['Cluster'])
            df_test['Cluster'] = new_clusters
    
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà HISTORICAL_ONLY_MODE ‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å
    if not HISTORICAL_ONLY_MODE:
        mask_unk = df_test['Cluster']=="UNK"
        if mask_unk.any():
            mask_geo = (df_test['Lat']!=0) & mask_unk
            if mask_geo.any():
                coords = np.radians(df_test.loc[mask_geo, ['Lat','Lon']].values)
                db = DBSCAN(eps=MAX_KM_CLUSTER/6371.0, min_samples=1).fit(coords)
                df_test.loc[mask_geo, 'Cluster'] = [f"G-{x}" if x!=-1 else "NOISE" for x in db.labels_]
            
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏û‡∏¥‡∏Å‡∏±‡∏î ‡πÉ‡∏´‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ï‡∏≤‡∏° prefix ‡∏Ç‡∏≠‡∏á Code
            mask_no_geo = (df_test['Lat']==0) & mask_unk
            if mask_no_geo.any():
                def get_code_prefix(code):
                    # ‡∏î‡∏∂‡∏á prefix ‡∏à‡∏≤‡∏Å‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô ZS, N, M, P)
                    code_str = str(code)
                    if len(code_str) >= 2:
                        # ‡∏ñ‡πâ‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ 2-3 ‡∏ï‡∏±‡∏ß
                        prefix = ''.join([c for c in code_str[:3] if c.isalpha()])
                        return f"PREFIX-{prefix}" if prefix else f"CODE-{code_str[:2]}"
                    return f"SINGLE-{code_str}"
                
                df_test.loc[mask_no_geo, 'Cluster'] = df_test.loc[mask_no_geo, 'Code'].apply(get_code_prefix)
        
        mask_fin = df_test['Cluster'].isin(["UNK","NOISE"])
        if mask_fin.any():
            df_test.loc[mask_fin, 'Cluster'] = df_test.loc[mask_fin, 'Code'].map(
                lambda x: f"Z-{region_map.get(x, 'NEW')}" if x in region_map else f"NEW-{x}"
            )
    
    final_rows = []
    trip_cnt = 1
    
    for cid, group in df_test.groupby('Cluster'):
        pool = []
        for code, sub in group.groupby('Code'):
            pool.append({
                'Code': code, 'Name': sub.iloc[0]['Name'],
                'Wgt': sub['Wgt'].sum(), 'Cube': sub['Cube'].sum(),
                'Lat': sub.iloc[0]['Lat'], 'Lon': sub.iloc[0]['Lon']
            })
            
        while pool:
            pool.sort(key=lambda x: x['Cube'], reverse=True)
            current_truck = []
            seed = pool.pop(0)
            current_truck.append(seed)
            
            curr_w = seed['Wgt']
            curr_c = seed['Cube']
            last_lat = seed['Lat']
            last_lon = seed['Lon']
            last_name = seed['Name']
            drops = 1
            max_req = constraints.get(seed['Code'], 1)
            
            while True:
                best_idx = -1
                best_score = float('inf')
                best_is_same_name = False
                
                for i, cand in enumerate(pool):
                    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î HISTORICAL_ONLY_MODE)
                    if HISTORICAL_ONLY_MODE:
                        # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö seed ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏±‡∏ö‡∏™‡∏≤‡∏Ç‡∏≤‡πÉ‡∏î‡πÜ ‡πÉ‡∏ô‡∏£‡∏ñ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                        has_history = False
                        for truck_item in current_truck:
                            if G.has_edge(truck_item['Code'], cand['Code']):
                                has_history = True
                                break
                        
                        if not has_history:
                            continue
                    
                    if STRICT_ZONE_MODE:
                        if last_lat != 0 and cand['Lat'] != 0:
                            zone_dist = haversine(last_lat, last_lon, cand['Lat'], cand['Lon'])
                            if zone_dist > MAX_ZONE_DISTANCE:
                                continue
                        
                        if not is_same_zone(seed['Code'], cand['Code'], region_map, geo):
                            continue
                    
                    new_w = curr_w + cand['Wgt']
                    new_c = curr_c + cand['Cube']
                    
                    if new_w > 5800: continue
                    if new_c > 22.0 * BUFFER: continue
                    
                    is_same_name = is_similar_name(last_name, cand['Name'])
                    dist = haversine(last_lat, last_lon, cand['Lat'], cand['Lon']) if last_lat!=0 and cand['Lat']!=0 else 999
                    is_nearby = (dist <= NEARBY_RADIUS)
                    
                    if drops >= TARGET_DROPS:
                        if drops >= MAX_DROPS_FLEX: 
                            continue
                        if not (is_same_name or is_nearby): 
                            continue
                    
                    score = dist
                    if is_same_name:
                        score -= 1000
                    
                    is_better = (score < best_score)
                    if is_same_name and not best_is_same_name:
                        is_better = True
                    elif best_is_same_name and not is_same_name:
                        is_better = False
                    
                    if is_better:
                        best_score = score
                        best_idx = i
                        best_is_same_name = is_same_name
                        
                if best_idx != -1:
                    sel = pool.pop(best_idx)
                    current_truck.append(sel)
                    
                    curr_w += sel['Wgt']
                    curr_c += sel['Cube']
                    drops += 1
                    
                    if sel['Lat']!=0: 
                        last_lat = sel['Lat']; last_lon = sel['Lon']
                    last_name = sel['Name']
                    max_req = max(max_req, constraints.get(sel['Code'], 1))
                else:
                    break
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á
            total_distance = 0
            for i in range(len(current_truck) - 1):
                if current_truck[i]['Lat'] != 0 and current_truck[i+1]['Lat'] != 0:
                    total_distance += haversine(
                        current_truck[i]['Lat'], current_truck[i]['Lon'],
                        current_truck[i+1]['Lat'], current_truck[i+1]['Lon']
                    )
            avg_distance = total_distance / max(1, len(current_truck) - 1)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì cube utilization ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            # ‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏£‡∏ñ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ
            if curr_w <= LIMITS['4W']['max_w'] and curr_c <= LIMITS['4W']['max_c']:
                cube_util = curr_c / LIMITS['4W']['max_c']
            elif curr_w <= LIMITS['JB']['max_w'] and curr_c <= LIMITS['JB']['max_c']:
                cube_util = curr_c / LIMITS['JB']['max_c']
            else:
                cube_util = curr_c / LIMITS['6W']['max_c']
            
            # ‡∏ñ‡πâ‡∏≤ utilization ‡∏¢‡∏±‡∏á‡∏ï‡πà‡∏≥ (<90%) ‡πÅ‡∏•‡∏∞‡∏¢‡∏±‡∏á‡∏°‡∏µ pool ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ‚Üí ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏™‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°
            # ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô MAX_DROPS_FLEX (12 ‡∏à‡∏∏‡∏î)
            # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Å‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 20 ‡∏£‡∏≠‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤
            max_scan = min(20, len(pool))
            if cube_util < MIN_CUBE_UTILIZATION and len(pool) > 0 and drops < MAX_DROPS_FLEX:
                # ‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏™‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î
                # ‡πÉ‡∏ä‡πâ while loop ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö index ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                i = 0
                scan_count = 0
                while i < len(pool) and cube_util < MIN_CUBE_UTILIZATION and drops < MAX_DROPS_FLEX and scan_count < max_scan:
                    scan_count += 1
                    cand = pool[i]
                    
                    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÉ‡∏™‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°
                    new_w_test = curr_w + cand['Wgt']
                    new_c_test = curr_c + cand['Cube']
                    
                    can_add = False
                    
                    # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏™‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 6W limit ‡πÅ‡∏•‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô zone ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
                    if new_w_test <= 5800 and new_c_test <= 22.0 * FLEX_CUBE_UTILIZATION:
                        can_add = True
                        
                        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡πà‡∏≠‡∏ô (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î HISTORICAL_ONLY_MODE)
                        if HISTORICAL_ONLY_MODE and can_add:
                            has_history = False
                            for truck_item in current_truck:
                                if G.has_edge(truck_item['Code'], cand['Code']):
                                    has_history = True
                                    break
                            if not has_history:
                                can_add = False
                        
                        # ‡πÄ‡∏ä‡πá‡∏Ñ zone ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î
                        if STRICT_ZONE_MODE and can_add:
                            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏à‡∏∏‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
                            if last_lat != 0 and cand['Lat'] != 0:
                                zone_dist = haversine(last_lat, last_lon, cand['Lat'], cand['Lon'])
                                if zone_dist > MAX_ZONE_DISTANCE:
                                    can_add = False
                            
                            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà zone ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö seed (‡∏à‡∏∏‡∏î‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡∏£‡∏ñ)
                            if can_add:
                                # ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà region ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö seed
                                seed_region = region_map.get(seed['Code'], 'UNKNOWN')
                                cand_region = region_map.get(cand['Code'], 'UNKNOWN')
                                
                                if seed_region != 'UNKNOWN' and cand_region != 'UNKNOWN':
                                    if seed_region != cand_region:
                                        can_add = False
                                
                                # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡∏à‡∏≤‡∏Å seed ‡∏î‡πâ‡∏ß‡∏¢
                                if can_add and seed['Lat'] != 0 and cand['Lat'] != 0:
                                    seed_dist = haversine(seed['Lat'], seed['Lon'], cand['Lat'], cand['Lon'])
                                    if seed_dist > MAX_ZONE_DISTANCE:
                                        can_add = False
                    
                    if can_add:
                        # ‡πÉ‡∏™‡πà‡πÄ‡∏û‡∏¥‡πà‡∏° - pop ‡∏à‡∏≤‡∏Å index ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                        sel_extra = pool.pop(i)
                        current_truck.append(sel_extra)
                        curr_w = new_w_test
                        curr_c = new_c_test
                        drops += 1
                        
                        if sel_extra['Lat'] != 0:
                            last_lat = sel_extra['Lat']
                            last_lon = sel_extra['Lon']
                        last_name = sel_extra['Name']
                        max_req = max(max_req, constraints.get(sel_extra['Code'], 1))
                        
                        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì utilization ‡πÉ‡∏´‡∏°‡πà
                        if curr_w <= LIMITS['4W']['max_w'] and curr_c <= LIMITS['4W']['max_c']:
                            cube_util = curr_c / LIMITS['4W']['max_c']
                        elif curr_w <= LIMITS['JB']['max_w'] and curr_c <= LIMITS['JB']['max_c']:
                            cube_util = curr_c / LIMITS['JB']['max_c']
                        else:
                            cube_util = curr_c / LIMITS['6W']['max_c']
                        
                        # ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏¥‡πà‡∏° i ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ pop ‡πÅ‡∏•‡πâ‡∏ß item ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏à‡∏∞‡∏°‡∏≤‡∏ó‡∏µ‡πà index ‡πÄ‡∏î‡∏¥‡∏°
                    else:
                        # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏™‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏ï‡∏±‡∏ß‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                        i += 1
            
            v_type = select_truck(curr_w, curr_c, max_req, avg_distance, cube_util)
            tid = f"AI-{trip_cnt:03d}"
            
            for item in current_truck:
                final_rows.append({
                    'Booking No': tid, '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏£‡∏ñ': v_type,
                    '‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤': item['Code'], '‡∏™‡∏≤‡∏Ç‡∏≤': item['Name'],
                    'TOTALWGT': item['Wgt'], 'TOTALCUBE': item['Cube'],
                    'Remark': f"Drops:{drops}", 'Lat': item['Lat'], 'Lon': item['Lon']
                })
            trip_cnt += 1
            
    return pd.DataFrame(final_rows)

def analyze_branch_groups(df_result, G):
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏™‡∏≤‡∏Ç‡∏≤‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
    ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÉ‡∏î‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
    """
    branch_groups = []
    
    for booking_no, group in df_result.groupby('Booking No'):
        codes = group['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤'].tolist()
        
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏™‡∏≤‡∏Ç‡∏≤‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ô‡∏µ‡πâ‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        historical_match = False
        if len(codes) > 1:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏π‡πà‡∏Ç‡∏≠‡∏á‡∏™‡∏≤‡∏Ç‡∏≤‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏°‡∏µ edge ‡πÉ‡∏ô Graph ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            all_paired = True
            for i in range(len(codes)):
                for j in range(i+1, len(codes)):
                    if not G.has_edge(codes[i], codes[j]):
                        all_paired = False
                        break
                if not all_paired:
                    break
            
            historical_match = all_paired
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡∏∏‡πà‡∏°
        group_info = {
            'Booking No': booking_no,
            '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏£‡∏ñ': group['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏£‡∏ñ'].iloc[0],
            '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏≤‡∏Ç‡∏≤': len(codes),
            '‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏Ç‡∏≤': ', '.join(codes),
            '‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏Ç‡∏≤‡πÄ‡∏ï‡πá‡∏°': '\n'.join([f"{code}: {name}" for code, name in zip(group['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤'], group['‡∏™‡∏≤‡∏Ç‡∏≤'])]),
            '‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏£‡∏ß‡∏°': group['TOTALWGT'].sum(),
            '‡∏Ñ‡∏¥‡∏ß‡∏£‡∏ß‡∏°': group['TOTALCUBE'].sum(),
            'Drops': group['Remark'].iloc[0] if 'Remark' in group.columns else f"Drops:{len(codes)}",
            '‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥': '‡πÉ‡∏ä‡πà ‚úì' if historical_match else '‡πÑ‡∏°‡πà ‚úó'
        }
        
        branch_groups.append(group_info)
    
    return pd.DataFrame(branch_groups)

def export_styled_excel(df, filename, df_groups=None):
    try:
        import xlsxwriter
        writer = pd.ExcelWriter(filename, engine='xlsxwriter')
        
        # ‡πÅ‡∏ó‡πá‡∏ö 1: ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏™‡∏≤‡∏Ç‡∏≤ (Branch Groups)
        if df_groups is not None:
            df_groups.to_excel(writer, index=False, sheet_name='Branch Groups')
            wb = writer.book
            ws_groups = writer.sheets['Branch Groups']
            
            # Format headers
            fmt_h = wb.add_format({
                'bold': True, 
                'bg_color': '#4472C4', 
                'font_color': 'white', 
                'border': 1,
                'text_wrap': True,
                'align': 'center',
                'valign': 'vcenter'
            })
            
            # Format for historical match (Yes)
            fmt_yes = wb.add_format({
                'bg_color': '#C6EFCE', 
                'font_color': '#006100',
                'border': 1,
                'align': 'center'
            })
            
            # Format for historical match (No)
            fmt_no = wb.add_format({
                'bg_color': '#FFC7CE', 
                'font_color': '#9C0006',
                'border': 1,
                'align': 'center'
            })
            
            # Format for normal cells
            fmt_normal = wb.add_format({'border': 1, 'text_wrap': True, 'valign': 'top'})
            fmt_number = wb.add_format({'border': 1, 'num_format': '#,##0.00'})
            
            # Write headers
            for c, val in enumerate(df_groups.columns):
                ws_groups.write(0, c, val, fmt_h)
            
            # Write data with conditional formatting
            for r, row in df_groups.iterrows():
                for c, (col_name, val) in enumerate(row.items()):
                    if col_name == '‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥':
                        if '‡πÉ‡∏ä‡πà' in str(val):
                            ws_groups.write(r+1, c, val, fmt_yes)
                        else:
                            ws_groups.write(r+1, c, val, fmt_no)
                    elif col_name in ['‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏£‡∏ß‡∏°', '‡∏Ñ‡∏¥‡∏ß‡∏£‡∏ß‡∏°']:
                        ws_groups.write(r+1, c, val, fmt_number)
                    else:
                        ws_groups.write(r+1, c, val, fmt_normal)
            
            # Adjust column widths
            ws_groups.set_column('A:A', 15)  # Booking No
            ws_groups.set_column('B:B', 18)  # ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏£‡∏ñ
            ws_groups.set_column('C:C', 12)  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏≤‡∏Ç‡∏≤
            ws_groups.set_column('D:D', 30)  # ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏Ç‡∏≤
            ws_groups.set_column('E:E', 40)  # ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏Ç‡∏≤‡πÄ‡∏ï‡πá‡∏°
            ws_groups.set_column('F:F', 15)  # ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏£‡∏ß‡∏°
            ws_groups.set_column('G:G', 12)  # ‡∏Ñ‡∏¥‡∏ß‡∏£‡∏ß‡∏°
            ws_groups.set_column('H:H', 15)  # Drops
            ws_groups.set_column('I:I', 20)  # ‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
        
        # ‡πÅ‡∏ó‡πá‡∏ö 2: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏£‡∏¥‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏° (Plan)
        df.to_excel(writer, index=False, sheet_name='Plan')
        wb = writer.book
        ws = writer.sheets['Plan']
        
        fmt_h = wb.add_format({'bold': True, 'bg_color': '#4472C4', 'font_color': 'white', 'border': 1})
        fmt_1 = wb.add_format({'bg_color': '#FFFFFF', 'border': 1})
        fmt_2 = wb.add_format({'bg_color': '#D9D9D9', 'border': 1})
        
        for c, val in enumerate(df.columns):
            ws.write(0, c, val, fmt_h)
        
        curr = None
        toggle = False
        for r, row in df.iterrows():
            if row['Booking No'] != curr:
                toggle = not toggle
                curr = row['Booking No']
            fmt = fmt_1 if toggle else fmt_2
            for c, val in enumerate(row):
                ws.write(r+1, c, val, fmt)
        
        writer.close()
    except Exception as e:
        st.error(f"‚ùå Error exporting Excel: {str(e)}")
        # Fallback to simple export
        df.to_excel(filename, index=False)

# ==========================================
# 5. STREAMLIT UI
# ==========================================
def main():
    st.set_page_config(page_title="AI Logistics Planner", page_icon="üöö", layout="wide")
    
    st.title("üöö AI Logistics Planner")
    st.markdown("---")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå DC (‡∏ã‡πà‡∏≠‡∏ô‡πÑ‡∏ß‡πâ)
    dc_folder = os.path.join(os.getcwd(), 'DC')
    dc_files_found = []
    if os.path.exists(dc_folder):
        dc_files_found = glob.glob(os.path.join(dc_folder, '*.xlsx')) + glob.glob(os.path.join(dc_folder, '*.xls'))
    
    # File uploader - ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Test
    st.subheader("üéØ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏≠‡πÄ‡∏î‡∏≠‡∏£‡πå (Test)")
    
    # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡πÉ‡∏ô session state (‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå)
    if 'last_uploaded_info' not in st.session_state:
        st.session_state.last_uploaded_info = None
    if 'result_ready' not in st.session_state:
        st.session_state.result_ready = False
    
    test_file = st.file_uploader("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå Test ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô", type=['xlsx', 'xls'], key='test')
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î)
    if test_file is not None:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á signature ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠ + ‡∏Ç‡∏ô‡∏≤‡∏î + ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        current_file_info = f"{test_file.name}_{test_file.size}_{test_file.tell()}"
        
        # ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î (‡πÅ‡∏°‡πâ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏¥‡∏°)
        if not st.session_state.result_ready or st.session_state.last_uploaded_info != current_file_info:
            st.session_state.last_uploaded_info = current_file_info
            st.session_state.result_ready = False
            st.cache_data.clear()
            st.success(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: {test_file.name}")
    elif test_file is None:
        # ‡∏ñ‡πâ‡∏≤‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏≠‡∏Å ‡πÉ‡∏´‡πâ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå session
        if st.session_state.last_uploaded_info is not None:
            st.session_state.last_uploaded_info = None
            st.session_state.result_ready = False
            st.cache_data.clear()
    
    st.markdown("---")
    
    if st.button("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô", type="primary", use_container_width=True):
        if not test_file:
            st.error("‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Test")
            return
        
        with st.spinner("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•..."):
            # Load training data ‡∏à‡∏≤‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå DC (‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡πÜ)
            tr_dfs = []
            
            if dc_files_found:
                for dc_file_path in dc_files_found:
                    try:
                        with open(dc_file_path, 'rb') as f:
                            file_content = f.read()
                            train_df = process_dataframe(load_excel(file_content))
                            if train_df is not None:
                                tr_dfs.append(train_df)
                    except:
                        pass
            
            # Train AI
            G, const, regions, learning_stats = train_ai(tr_dfs)
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ (‡∏ã‡πà‡∏≠‡∏ô‡πÑ‡∏ß‡πâ)
            with st.expander("üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥", expanded=False):
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("üöö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏£‡∏¥‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ", f"{learning_stats['total_trips']}")
                with col2:
                    st.metric("üè™ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", f"{learning_stats['total_branches']}")
                with col3:
                    st.metric("üìç ‡∏à‡∏∏‡∏î‡∏™‡πà‡∏á‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢/‡∏ó‡∏£‡∏¥‡∏õ", f"{learning_stats['avg_drops']:.1f}")
                with col4:
                    st.metric("üó∫Ô∏è ‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢", f"{learning_stats['avg_distance']:.0f} km")
                
                st.markdown("---")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.write("**üåè ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ:**")
                    for region, count in sorted(learning_stats['region_distribution'].items(), key=lambda x: x[1], reverse=True):
                        if region != 'UNKNOWN':
                            st.write(f"- {region}: {count} ‡∏ó‡∏£‡∏¥‡∏õ")
                
                with col2:
                    st.write("**üöõ ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏£‡∏ñ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó:**")
                    for veh, count in sorted(learning_stats['vehicle_usage'].items(), key=lambda x: x[1], reverse=True):
                        st.write(f"- {veh}: {count} ‡∏ó‡∏£‡∏¥‡∏õ")
                
                st.info(f"üí° ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏ñ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥")
            
            # Load geo - ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å training files
            geo = {}
            for df in tr_dfs:
                if df is not None:
                    temp_geo = process_geo(df)
                    geo.update(temp_geo)
            
            # Process test data
            test_content = test_file.read()
            df_test = process_dataframe(load_excel(test_content))
            if df_test is None:
                st.error("‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Test")
                return
            
            # ‡∏î‡∏∂‡∏á‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏à‡∏≤‡∏Å‡∏ä‡∏µ‡∏ï Location ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå Test (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            test_file.seek(0)  # reset file pointer
            df_location = load_excel(test_file.read(), sheet_name='Location')
            if df_location is not None:
                df_location_processed = process_dataframe(df_location)
                if df_location_processed is not None:
                    location_geo = process_geo(df_location_processed)
                    if location_geo:
                        geo.update(location_geo)
            
            # Run prediction
            res = run_prediction(df_test, G, geo, const, regions)
            
            # ‡∏õ‡∏¥‡∏î Post-processing: merge_small_trips ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
            # res = merge_small_trips(res, geo, regions)
            
            res = res.sort_values(by=['Booking No', 'Lat'])
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏•‡πâ‡∏ß
            st.session_state.result_ready = True
            
            # Display results
            total_trips = res['Booking No'].nunique()
            trip_summary = res.groupby('Booking No').agg({
                '‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤': 'count',
                'TOTALWGT': 'sum',
                'TOTALCUBE': 'sum'
            }).rename(columns={'‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤': 'Drops'})
            
            st.markdown("---")
            st.success("### ‚úÖ ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üöö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß", f"{total_trips} ‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß")
            with col2:
                st.metric("üìç ‡∏à‡∏∏‡∏î‡∏™‡πà‡∏á‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢", f"{trip_summary['Drops'].mean():.1f} ‡∏à‡∏∏‡∏î/‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß")
            with col3:
                st.metric("‚öñÔ∏è ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢", f"{trip_summary['TOTALWGT'].mean():.0f} kg/‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß")
            with col4:
                st.metric("üì¶ ‡∏Ñ‡∏¥‡∏ß‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢", f"{trip_summary['TOTALCUBE'].mean():.2f} cbm/‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß")
            
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏™‡∏≤‡∏Ç‡∏≤
            df_groups = analyze_branch_groups(res, G)
            
            # Display tabs
            st.markdown("---")
            tab1, tab2 = st.tabs(["üìä ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏™‡∏≤‡∏Ç‡∏≤ (Branch Groups)", "üìã ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏£‡∏¥‡∏õ (Plan)"])
            
            with tab1:
                st.subheader("üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥")
                st.markdown("""
                **‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
                - ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤
                - ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏à‡∏±‡∏î‡∏ó‡∏£‡∏¥‡∏õ‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (Historical-Based Routing)
                """)
                
                # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
                df_groups_historical = df_groups[df_groups['‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥'].str.contains('‡πÉ‡∏ä‡πà')].copy()
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
                total_groups = len(df_groups)
                historical_groups = len(df_groups_historical)
                new_groups = total_groups - historical_groups
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("üì¶ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", total_groups)
                with col2:
                    st.metric("‚úÖ ‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥", historical_groups)
                with col3:
                    st.metric("üÜï ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÉ‡∏´‡∏°‡πà", new_groups)
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô)
                if len(df_groups_historical) > 0:
                    st.dataframe(
                        df_groups_historical,
                        use_container_width=True,
                        height=400,
                    column_config={
                        'Booking No': st.column_config.TextColumn('Booking No', width='small'),
                        '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏£‡∏ñ': st.column_config.TextColumn('‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏£‡∏ñ', width='medium'),
                        '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏≤‡∏Ç‡∏≤': st.column_config.NumberColumn('‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏≤‡∏Ç‡∏≤', width='small'),
                        '‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏Ç‡∏≤': st.column_config.TextColumn('‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏Ç‡∏≤ (‡∏£‡∏´‡∏±‡∏™)', width='large'),
                        '‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏Ç‡∏≤‡πÄ‡∏ï‡πá‡∏°': st.column_config.TextColumn('‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏Ç‡∏≤ (‡πÄ‡∏ï‡πá‡∏°)', width='large'),
                        '‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏£‡∏ß‡∏°': st.column_config.NumberColumn('‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏£‡∏ß‡∏° (kg)', format='%.2f'),
                        '‡∏Ñ‡∏¥‡∏ß‡∏£‡∏ß‡∏°': st.column_config.NumberColumn('‡∏Ñ‡∏¥‡∏ß‡∏£‡∏ß‡∏° (cbm)', format='%.2f'),
                        'Drops': st.column_config.TextColumn('Drops', width='small'),
                        '‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥': st.column_config.TextColumn('‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥', width='medium')
                    }
                )
                
                else:
                    st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≤‡∏Ç‡∏≤‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                if new_groups > 0:
                    st.info(f"‚ÑπÔ∏è ‡∏°‡∏µ {new_groups} ‡∏ó‡∏£‡∏¥‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≤‡∏Ç‡∏≤‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥ (‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÉ‡∏ô‡πÅ‡∏ó‡πá‡∏ö Plan)")
                    
                    new_group_list = df_groups[df_groups['‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥'].str.contains('‡πÑ‡∏°‡πà')]
                    with st.expander("üìã ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏≤‡∏Ç‡∏≤‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß/‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥"):
                        for _, row in new_group_list.iterrows():
                            st.markdown(f"""
                            **{row['Booking No']}** ({row['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏£‡∏ñ']})
                            - ‡∏™‡∏≤‡∏Ç‡∏≤: {row['‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏Ç‡∏≤']}
                            - ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å: {row['‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏£‡∏ß‡∏°']:.2f} kg
                            - ‡∏Ñ‡∏¥‡∏ß: {row['‡∏Ñ‡∏¥‡∏ß‡∏£‡∏ß‡∏°']:.2f} cbm
                            """)
            
            with tab2:
                st.subheader("üìã ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏£‡∏¥‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏°")
                st.dataframe(res, use_container_width=True, height=400)
            
            # Export
            output_filename = 'AI_Sticky_Routing_Plan.xlsx'
            export_styled_excel(res, output_filename, df_groups)
            
            with open(output_filename, 'rb') as f:
                st.download_button(
                    label="üíæ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel (2 ‡πÅ‡∏ó‡πá‡∏ö)",
                    data=f,
                    file_name=output_filename,
                    mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                    use_container_width=True
                )
            
            st.balloons()

if __name__ == "__main__":
    main()
