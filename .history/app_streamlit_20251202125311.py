import streamlit as st
import pandas as pd
import numpy as np
import io
import os
import glob
import networkx as nx
from sklearn.cluster import DBSCAN
import math
import warnings

warnings.filterwarnings('ignore')

# ==========================================
# 1. CONFIG
# ==========================================
LIMITS = {'4W': {'max_w': 2500, 'max_c': 5.0}, 'JB': {'max_w': 3500, 'max_c': 8.0}, '6W': {'max_w': 5800, 'max_c': 22.0}}
BUFFER = 1.05
MAX_KM_CLUSTER = 30.0
TARGET_DROPS = 10
MAX_DROPS_FLEX = 12
NEARBY_RADIUS = 5.0
MAX_ZONE_DISTANCE = 100.0
STRICT_ZONE_MODE = True

# Utilization thresholds for truck optimization
MIN_CUBE_UTILIZATION = 0.90  # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≥ 90% ‡∏Å‡πà‡∏≠‡∏ô‡∏õ‡∏¥‡∏î‡∏£‡∏ñ
TARGET_CUBE_UTILIZATION = 1.00  # ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ 100%
FLEX_CUBE_UTILIZATION = 1.05  # ‡∏¢‡∏≠‡∏°‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏î‡πâ‡∏ñ‡∏∂‡∏á 105%

EXCLUDE = ['PTDC', 'Distribution Center', 'DC‡∏ß‡∏±‡∏á‡∏ô‡πâ‡∏≠‡∏¢', 'DC011']

# ==========================================
# 2. HELPER FUNCTIONS
# ==========================================
def normalize(val):
    return str(val).strip().upper().replace(" ", "").replace(".0", "")

def haversine(lat1, lon1, lat2, lon2):
    R = 6371
    dLat = math.radians(lat2 - lat1)
    dLon = math.radians(lon2 - lon1)
    a = math.sin(dLat/2) * math.sin(dLat/2) + \
        math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * \
        math.sin(dLon/2) * math.sin(dLon/2)
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
    return R * c

def is_similar_name(name1, name2):
    def clean(n):
        return ''.join([c for c in str(n) if c.isalpha()])
    return clean(name1) == clean(name2) and len(clean(name1)) > 3

def get_province_zone(province):
    if not province or pd.isna(province):
        return 'UNKNOWN'
    
    prov = str(province).strip()
    
    central = ['‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û', '‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ', '‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ', '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£', '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏≤‡∏Ñ‡∏£', '‡∏ô‡∏Ñ‡∏£‡∏õ‡∏ê‡∏°', 
               '‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°', '‡∏£‡∏≤‡∏ä‡∏ö‡∏∏‡∏£‡∏µ', '‡∏Å‡∏≤‡∏ç‡∏à‡∏ô‡∏ö‡∏∏‡∏£‡∏µ', '‡∏™‡∏∏‡∏û‡∏£‡∏£‡∏ì‡∏ö‡∏∏‡∏£‡∏µ', '‡∏ä‡∏±‡∏¢‡∏ô‡∏≤‡∏ó', '‡∏™‡∏¥‡∏á‡∏´‡πå‡∏ö‡∏∏‡∏£‡∏µ', 
               '‡∏≠‡πà‡∏≤‡∏á‡∏ó‡∏≠‡∏á', '‡∏•‡∏û‡∏ö‡∏∏‡∏£‡∏µ', '‡∏™‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏µ', '‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤', '‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤']
    
    northeast = ['‡∏ô‡∏Ñ‡∏£‡∏£‡∏≤‡∏ä‡∏™‡∏µ‡∏°‡∏≤', '‡πÇ‡∏Ñ‡∏£‡∏≤‡∏ä', '‡∏ö‡∏∏‡∏£‡∏µ‡∏£‡∏±‡∏°‡∏¢‡πå', '‡∏™‡∏∏‡∏£‡∏¥‡∏ô‡∏ó‡∏£‡πå', '‡∏®‡∏µ‡∏Ç‡∏£‡∏†‡∏π‡∏°‡∏¥', '‡∏Ç‡∏≠‡∏ô‡πÅ‡∏Å‡πà‡∏ô', 
                 '‡∏≠‡∏∏‡∏î‡∏£‡∏ò‡∏≤‡∏ô‡∏µ', '‡πÄ‡∏•‡∏¢', '‡∏´‡∏ô‡∏≠‡∏á‡∏Ñ‡∏≤‡∏¢', '‡∏°‡∏´‡∏≤‡∏™‡∏≤‡∏£‡∏Ñ‡∏≤‡∏°', '‡∏£‡πâ‡∏≠‡∏¢‡πÄ‡∏≠‡πá‡∏î', '‡∏Å‡∏≤‡∏¨‡∏™‡∏¥‡∏ô‡∏ò‡∏∏‡πå', 
                 '‡∏™‡∏Å‡∏•‡∏ô‡∏Ñ‡∏£', '‡∏ô‡∏Ñ‡∏£‡∏û‡∏ô‡∏°', '‡∏°‡∏∏‡∏Å‡∏î‡∏≤‡∏´‡∏≤‡∏£', '‡∏¢‡πÇ‡∏™‡∏ò‡∏£', '‡∏≠‡∏≥‡∏ô‡∏≤‡∏à‡πÄ‡∏à‡∏£‡∏¥‡∏ç', '‡∏≠‡∏∏‡∏ö‡∏•‡∏£‡∏≤‡∏ä‡∏ò‡∏≤‡∏ô‡∏µ', 
                 '‡∏ä‡∏±‡∏¢‡∏†‡∏π‡∏°‡∏¥', '‡∏ö‡∏∂‡∏á‡∏Å‡∏≤‡∏¨']
    
    north = ['‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà', '‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡∏£‡∏≤‡∏¢', '‡∏•‡∏≥‡∏û‡∏π‡∏ô', '‡∏•‡∏≥‡∏õ‡∏≤‡∏á', '‡∏û‡∏∞‡πÄ‡∏¢‡∏≤', '‡πÅ‡∏û‡∏£‡πà', '‡∏ô‡πà‡∏≤‡∏ô', 
             '‡∏≠‡∏∏‡∏ï‡∏£‡∏î‡∏¥‡∏ï‡∏ñ‡πå', '‡∏ï‡∏≤‡∏Å', '‡∏™‡∏∏‡πÇ‡∏Ç‡∏ó‡∏±‡∏¢', '‡∏û‡∏¥‡∏©‡∏ì‡∏∏‡πÇ‡∏•‡∏Å', '‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£', '‡πÄ‡∏û‡∏ä‡∏£‡∏ö‡∏π‡∏£‡∏ì‡πå', '‡∏Å‡∏≥‡πÅ‡∏û‡∏á‡πÄ‡∏û‡∏ä‡∏£']
    
    south = ['‡∏ä‡∏∏‡∏°‡∏û‡∏£', '‡∏™‡∏∏‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ò‡∏≤‡∏ô‡∏µ', '‡∏£‡∏∞‡∏ô‡∏≠‡∏á', '‡∏û‡∏±‡∏á‡∏á‡∏≤', '‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï', '‡∏Å‡∏£‡∏∞‡∏ö‡∏µ‡πà', '‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏ò‡∏£‡∏£‡∏°‡∏£‡∏≤‡∏ä', 
             '‡∏ï‡∏£‡∏±‡∏á', '‡∏û‡∏±‡∏ó‡∏•‡∏∏‡∏á', '‡∏™‡∏á‡∏Ç‡∏•‡∏≤', '‡∏™‡∏ï‡∏π‡∏•', '‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ', '‡∏¢‡∏∞‡∏•‡∏≤', '‡∏ô‡∏£‡∏≤‡∏ò‡∏¥‡∏ß‡∏≤‡∏™']
    
    east = ['‡∏â‡∏∞‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏ó‡∏£‡∏≤', '‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ', '‡∏£‡∏∞‡∏¢‡∏≠‡∏á', '‡∏à‡∏±‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ', '‡∏ï‡∏£‡∏≤‡∏î', '‡∏õ‡∏£‡∏≤‡∏à‡∏µ‡∏ô‡∏ö‡∏∏‡∏£‡∏µ', '‡∏™‡∏£‡∏∞‡πÅ‡∏Å‡πâ‡∏ß']
    
    west = ['‡∏Å‡∏≤‡∏ç‡∏à‡∏ô‡∏ö‡∏∏‡∏£‡∏µ', '‡∏ï‡∏≤‡∏Å', '‡∏õ‡∏£‡∏∞‡∏à‡∏ß‡∏ö‡∏Ñ‡∏µ‡∏£‡∏µ‡∏Ç‡∏±‡∏ô‡∏ò‡πå', '‡πÄ‡∏û‡∏ä‡∏£‡∏ö‡∏∏‡∏£‡∏µ']
    
    for p in central:
        if p in prov: return 'CENTRAL'
    for p in northeast:
        if p in prov: return 'NORTHEAST'
    for p in north:
        if p in prov: return 'NORTH'
    for p in south:
        if p in prov: return 'SOUTH'
    for p in east:
        if p in prov: return 'EAST'
    for p in west:
        if p in prov: return 'WEST'
    
    return 'UNKNOWN'

def is_same_zone(code1, code2, zone_map, geo):
    if not STRICT_ZONE_MODE:
        return True
    
    if code1 in geo and code2 in geo:
        lat1, lon1 = geo[code1]
        lat2, lon2 = geo[code2]
        if lat1 != 0 and lat2 != 0:
            dist = haversine(lat1, lon1, lat2, lon2)
            if dist > MAX_ZONE_DISTANCE:
                return False
    
    zone1 = zone_map.get(code1, 'UNKNOWN')
    zone2 = zone_map.get(code2, 'UNKNOWN')
    
    if zone1 != 'UNKNOWN' and zone2 != 'UNKNOWN':
        if zone1 != zone2:
            return False
    
    return True

# ==========================================
# 3. LOADERS & PROCESSORS
# ==========================================
def load_excel(content, sheet_name=None):
    try:
        xls = pd.ExcelFile(io.BytesIO(content))
        target_sheet = None
        
        # ‡∏ñ‡πâ‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏µ‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞
        if sheet_name:
            if sheet_name in xls.sheet_names:
                target_sheet = sheet_name
            else:
                # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏ä‡∏µ‡∏ï‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô
                for s in xls.sheet_names:
                    if sheet_name.lower() in s.lower():
                        target_sheet = s
                        break
        
        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏ä‡πâ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        if not target_sheet:
            priority = ['2.punthai', '2.', 'punthai', 'order', 'history', 'data', 'sheet']
            
            for p in priority:
                for s in xls.sheet_names:
                    if p in s.lower(): 
                        target_sheet = s
                        break
                if target_sheet: break
        
        if not target_sheet: target_sheet = xls.sheet_names[0]
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ header row ‡πÇ‡∏î‡∏¢‡∏î‡∏π‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î
        df_tmp = pd.read_excel(xls, sheet_name=target_sheet, nrows=30, header=None)
        h_row = -1
        
        keywords = ['CODE', 'BRANCH', '‡∏™‡∏≤‡∏Ç‡∏≤', 'WGT', 'CUBE', '‡∏Ñ‡∏¥‡∏ß', '‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å', 
                   'TRIP', 'BOOKING', '‡∏£‡∏´‡∏±‡∏™', '‡∏ó‡∏£‡∏¥‡∏õ', 'LAT', 'LON', 'VEHICLE']
        
        for i, r in df_tmp.iterrows():
            row_str = r.astype(str).str.upper().tolist()
            # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ß
            match_count = sum(1 for k in keywords if any(k in s for s in row_str))
            if match_count >= 3:  # ‡∏ñ‡πâ‡∏≤‡∏û‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3 ‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î = header
                h_row = i
                break
        
        if h_row == -1: h_row = 0  # ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏ä‡πâ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å
        
        df = pd.read_excel(xls, sheet_name=target_sheet, header=h_row)
        return df
    except Exception as e:
        st.error(f"‚ùå Error loading Excel sheet '{sheet_name}': {str(e)}")
        return None

def process_dataframe(df):
    if df is None: return None
    df.columns = df.columns.astype(str).str.strip()
    df = df.loc[:, ~df.columns.duplicated()]
    rename_map = {}
    for c in df.columns:
        cu = c.upper().replace(' ','').replace('_','')
        if 'BRANCHCODE' in cu or '‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤' in cu: rename_map[c] = 'Code'
        elif 'BRANCH' in cu or '‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏Ç‡∏≤' in cu or '‡∏™‡∏≤‡∏Ç‡∏≤'==c: rename_map[c] = 'Name'
        elif 'WGT' in cu or '‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å' in cu: rename_map[c] = 'Wgt'
        elif 'CUBE' in cu or '‡∏Ñ‡∏¥‡∏ß' in cu: rename_map[c] = 'Cube'
        elif 'LAT' in cu: rename_map[c] = 'Lat'
        elif 'LON' in cu: rename_map[c] = 'Lon'
        elif 'TRIP' in cu or 'BOOKING' in cu: rename_map[c] = 'Trip'
        elif 'VEHICLE' in cu or 'TRIPNO' in cu: rename_map[c] = 'Vehicle'
        elif '‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î' in cu: rename_map[c] = 'Province'
    
    df.rename(columns=rename_map, inplace=True)
    if 'Code' not in df.columns:
        if 'Name' in df.columns: df['Code'] = df['Name']
        else: return None
        
    df['Code'] = df['Code'].apply(normalize)
    for c in ['Wgt','Cube','Lat','Lon']:
        if c not in df.columns: df[c] = 0.0
        else: df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0.0)
        
    mask_ex = df['Code'].isin(EXCLUDE)
    if 'Name' in df.columns: mask_ex |= df['Name'].apply(lambda x: any(k in str(x) for k in EXCLUDE))
    return df[~mask_ex].copy()

def process_geo(df):
    if df is None: return {}
    # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á process_dataframe ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ df ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏ñ‡∏π‡∏Å process ‡πÅ‡∏•‡πâ‡∏ß
    geo = {}
    if df is not None and 'Code' in df.columns and 'Lat' in df.columns and 'Lon' in df.columns:
        for _, r in df.iterrows():
            if pd.notna(r['Lat']) and r['Lat'] != 0 and pd.notna(r['Code']):
                code = normalize(str(r['Code']))
                geo[code] = (float(r['Lat']), float(r['Lon']))
    return geo

# ==========================================
# 4. AI CORE
# ==========================================
def train_ai(df_list):
    G = nx.Graph()
    req = {}
    zones = {}
    regions = {}
    trip_distances = {}  # ‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ó‡∏£‡∏¥‡∏õ
    trip_patterns = []   # ‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î
    
    for df in df_list:
        if df is None or 'Trip' not in df.columns: continue
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á copy ‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ã‡πâ‡∏≥
        df = df.copy()
        df = df.loc[:, ~df.columns.duplicated()]
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Trip ‡πÄ‡∏õ‡πá‡∏ô DataFrame
        if isinstance(df['Trip'], pd.DataFrame):
            df['Trip'] = df['Trip'].iloc[:,0]
        
        # ‡πÅ‡∏õ‡∏•‡∏á Trip ‡πÄ‡∏õ‡πá‡∏ô string ‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        df['Trip'] = df['Trip'].astype(str)
        df = df[(df['Trip'].notna()) & (df['Trip'] != 'nan') & (df['Trip'] != '') & (df['Trip'] != 'None')]
        
        if len(df) == 0:
            continue
        
        # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡πÅ‡∏•‡∏∞‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ
        for idx, r in df.iterrows():
            if 'Province' in df.columns and pd.notna(r['Province']):
                prov = str(r['Province']).strip()
                zones[r['Code']] = prov
                regions[r['Code']] = get_province_zone(prov)
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏£‡∏¥‡∏õ
        for t, g in df.groupby('Trip'):
            codes = g['Code'].unique()
            veh = str(g['Vehicle'].iloc[0]).upper() if 'Vehicle' in g.columns else ''
            rank = 3 if '6' in veh else (2 if 'J' in veh else 1)
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å requirement ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡∏≤‡∏Ç‡∏≤
            for c in codes: 
                req[c] = max(req.get(c,1), rank)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏ó‡∏£‡∏¥‡∏õ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏û‡∏¥‡∏Å‡∏±‡∏î)
            if 'Lat' in g.columns and 'Lon' in g.columns:
                total_dist = 0
                coords = g[['Lat', 'Lon']].values
                for i in range(len(coords)-1):
                    if coords[i][0] != 0 and coords[i+1][0] != 0:
                        total_dist += haversine(coords[i][0], coords[i][1], 
                                               coords[i+1][0], coords[i+1][1])
                
                if total_dist > 0:
                    trip_distances[t] = total_dist
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏£‡∏¥‡∏õ
            trip_info = {
                'trip': t,
                'branches': len(codes),
                'vehicle': veh,
                'weight': g['Wgt'].sum() if 'Wgt' in g.columns else 0,
                'cube': g['Cube'].sum() if 'Cube' in g.columns else 0,
                'codes': list(codes),
                'region': regions.get(codes[0], 'UNKNOWN') if len(codes) > 0 else 'UNKNOWN'
            }
            trip_patterns.append(trip_info)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå (‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô)
            if len(codes)>1:
                for i in range(len(codes)):
                    for j in range(i+1, len(codes)): 
                        G.add_edge(codes[i], codes[j])
            elif len(codes)==1: 
                G.add_node(codes[0])
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
    learning_stats = {
        'total_trips': len(trip_patterns),
        'total_branches': len(req),
        'avg_drops': sum(p['branches'] for p in trip_patterns) / len(trip_patterns) if trip_patterns else 0,
        'avg_distance': sum(trip_distances.values()) / len(trip_distances) if trip_distances else 0,
        'region_distribution': {},
        'vehicle_usage': {}
    }
    
    # ‡∏ô‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ
    for pattern in trip_patterns:
        region = pattern['region']
        learning_stats['region_distribution'][region] = learning_stats['region_distribution'].get(region, 0) + 1
        
        veh = pattern['vehicle']
        if '6' in veh:
            veh_type = '6W'
        elif 'J' in veh or '‡∏à‡∏±‡∏°‡πÇ‡∏ö' in veh:
            veh_type = '4W-JB'
        else:
            veh_type = '4W'
        learning_stats['vehicle_usage'][veh_type] = learning_stats['vehicle_usage'].get(veh_type, 0) + 1
    
    return G, req, regions, learning_stats

def select_truck(w, c, min_rank, avg_distance=0, cube_utilization=0):
    """
    ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏ñ‡∏ï‡∏≤‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å ‡∏Ñ‡∏¥‡∏ß ‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
    
    ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå:
    1. ‡πÉ‡∏ä‡πâ 4W ‡∏ñ‡πâ‡∏≤‡πÉ‡∏™‡πà‡πÑ‡∏î‡πâ‡∏û‡∏≠‡∏î‡∏µ
    2. ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô 4W ‚Üí ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏ä‡πâ 4W Jumbo ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤ (>90% cube)
    3. ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ 6W ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ utilization ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å (>90%)
    4. ‡∏ñ‡πâ‡∏≤‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡πÑ‡∏Å‡∏•‡∏°‡∏≤‡∏Å (>150km) ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ 6W ‚Üí ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏ñ‡∏∂‡∏á 80% cube
    """
    
    # ‡∏ñ‡πâ‡∏≤ requirement ‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ 6W
    if min_rank >= 3:
        return '6 ‡∏•‡πâ‡∏≠ ‡∏ï‡∏π‡πâ‡∏ó‡∏∂‡∏ö'
    
    # 4W ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤: ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å ‚â§ 2500 kg ‡πÅ‡∏•‡∏∞ ‡∏Ñ‡∏¥‡∏ß ‚â§ 5.0
    if w <= LIMITS['4W']['max_w'] and c <= LIMITS['4W']['max_c']:
        return '4 ‡∏•‡πâ‡∏≠ ‡∏ï‡∏π‡πâ‡∏ó‡∏∂‡∏ö'
    
    # 4W ‡∏à‡∏±‡∏°‡πÇ‡∏ö‡πâ: ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å ‚â§ 3500 kg ‡πÅ‡∏•‡∏∞ ‡∏Ñ‡∏¥‡∏ß ‚â§ 8.0
    if w <= LIMITS['JB']['max_w'] and c <= LIMITS['JB']['max_c']:
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì utilization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 4W Jumbo
        jumbo_util = c / LIMITS['JB']['max_c']
        
        # ‡∏ñ‡πâ‡∏≤ utilization ‡∏î‡∏µ (>70%) ‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏ï‡πá‡∏° ‚Üí ‡πÉ‡∏ä‡πâ 4W Jumbo
        if jumbo_util >= 0.70 or w >= LIMITS['JB']['max_w'] * 0.80:
            return '4 ‡∏•‡πâ‡∏≠ ‡∏à‡∏±‡∏°‡πÇ‡∏ö‡πâ ‡∏ï‡∏π‡πâ‡∏ó‡∏∂‡∏ö'
        
        # ‡∏ñ‡πâ‡∏≤ utilization ‡∏ï‡πà‡∏≥ ‡πÅ‡∏ï‡πà‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏Å‡∏• (<100km) ‚Üí ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ 4W Jumbo ‡πÑ‡∏î‡πâ
        if avg_distance < 100:
            return '4 ‡∏•‡πâ‡∏≠ ‡∏à‡∏±‡∏°‡πÇ‡∏ö‡πâ ‡∏ï‡∏π‡πâ‡∏ó‡∏∂‡∏ö'
    
    # ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ 6W
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì utilization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 6W
    six_wheel_util = c / LIMITS['6W']['max_c']
    
    # ‡∏ñ‡πâ‡∏≤‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡πÑ‡∏Å‡∏•‡∏°‡∏≤‡∏Å (>150km) ‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ 6W ‚Üí ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö utilization ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤
    if avg_distance > 150:
        if six_wheel_util >= 0.80:  # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 80% ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏Å‡∏•
            return '6 ‡∏•‡πâ‡∏≠ ‡∏ï‡∏π‡πâ‡∏ó‡∏∂‡∏ö'
    
    # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ: 6W ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ utilization ‡∏™‡∏π‡∏á (>90%)
    if six_wheel_util >= MIN_CUBE_UTILIZATION:
        return '6 ‡∏•‡πâ‡∏≠ ‡∏ï‡∏π‡πâ‡∏ó‡∏∂‡∏ö'
    
    # ‡∏ñ‡πâ‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô 4W Jumbo ‡πÅ‡∏ï‡πà cube ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏ï‡πá‡∏° ‚Üí ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ 6W ‡πÅ‡∏ï‡πà‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô
    if w > LIMITS['JB']['max_w']:
        return '6 ‡∏•‡πâ‡∏≠ ‡∏ï‡∏π‡πâ‡∏ó‡∏∂‡∏ö'
    
    # Default: 4W Jumbo
    return '4 ‡∏•‡πâ‡∏≠ ‡∏à‡∏±‡∏°‡πÇ‡∏ö‡πâ ‡∏ï‡∏π‡πâ‡∏ó‡∏∂‡∏ö'

def merge_small_trips(df_result, geo, region_map):
    """‡∏£‡∏ß‡∏°‡∏ó‡∏£‡∏¥‡∏õ‡πÄ‡∏•‡πá‡∏Å‡πÜ (1-2 ‡∏à‡∏∏‡∏î) ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô"""
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ó‡∏£‡∏¥‡∏õ
    trip_stats = df_result.groupby('Booking No').agg({
        '‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤': 'count',
        'TOTALWGT': 'sum',
        'TOTALCUBE': 'sum'
    }).rename(columns={'‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤': 'drops'})
    
    # ‡∏´‡∏≤‡∏ó‡∏£‡∏¥‡∏õ‡πÄ‡∏•‡πá‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ (‚â§ 3 ‡∏à‡∏∏‡∏î, ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å < 1000 kg, ‡∏Ñ‡∏¥‡∏ß < 2.0)
    small_trips = trip_stats[(trip_stats['drops'] <= 3) & 
                            (trip_stats['TOTALWGT'] < 1000) & 
                            (trip_stats['TOTALCUBE'] < 2.0)].index.tolist()
    
    if not small_trips:
        return df_result
    
    # ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏£‡∏¥‡∏õ‡πÄ‡∏•‡πá‡∏Å‡∏ï‡∏≤‡∏° prefix
    trip_groups = {}
    for trip_id in small_trips:
        trip_data = df_result[df_result['Booking No'] == trip_id]
        # ‡∏î‡∏π‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤‡πÅ‡∏£‡∏Å
        first_code = trip_data.iloc[0]['‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤']
        prefix = ''.join([c for c in str(first_code)[:3] if c.isalpha()])
        
        if prefix not in trip_groups:
            trip_groups[prefix] = []
        trip_groups[prefix].append(trip_id)
    
    # ‡∏£‡∏ß‡∏°‡∏ó‡∏£‡∏¥‡∏õ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏°
    new_rows = []
    merged_trips = set()
    trip_counter = 1
    
    for prefix, trips in trip_groups.items():
        if len(trips) <= 1:
            continue
            
        # ‡∏£‡∏ß‡∏°‡∏ó‡∏£‡∏¥‡∏õ‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ô‡∏µ‡πâ
        combined_data = []
        total_w = 0
        total_c = 0
        
        for trip_id in trips:
            trip_data = df_result[df_result['Booking No'] == trip_id]
            for _, row in trip_data.iterrows():
                combined_data.append(row.to_dict())
                total_w += row['TOTALWGT']
                total_c += row['TOTALCUBE']
            merged_trips.add(trip_id)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î
        if total_w <= 5800 and total_c <= 22.0 * BUFFER and len(combined_data) <= MAX_DROPS_FLEX:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏£‡∏¥‡∏õ‡πÉ‡∏´‡∏°‡πà
            new_trip_id = f"AI-MERGED-{prefix}-{trip_counter}"
            trip_counter += 1
            
            for item in combined_data:
                item['Booking No'] = new_trip_id
                item['Remark'] = f"Drops:{len(combined_data)}"
                new_rows.append(item)
    
    # ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏£‡∏¥‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏ß‡∏°
    for _, row in df_result.iterrows():
        if row['Booking No'] not in merged_trips:
            new_rows.append(row.to_dict())
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö Booking No ‡πÉ‡∏´‡∏°‡πà
    if new_rows:
        df_merged = pd.DataFrame(new_rows)
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö Booking No ‡πÉ‡∏´‡∏°‡πà
        unique_bookings = sorted(df_merged['Booking No'].unique())
        booking_map = {old: f"AI-{i+1:03d}" for i, old in enumerate(unique_bookings)}
        df_merged['Booking No'] = df_merged['Booking No'].map(booking_map)
        
        return df_merged
    
    return df_result

def run_prediction(df_test, G, geo, constraints, region_map):
    df_test['Lat'] = df_test.apply(lambda r: geo.get(r['Code'],(0,0))[0] if r['Lat']==0 else r['Lat'], axis=1)
    df_test['Lon'] = df_test.apply(lambda r: geo.get(r['Code'],(0,0))[1] if r['Lon']==0 else r['Lon'], axis=1)
    df_test['Region'] = df_test['Code'].map(lambda x: region_map.get(x, 'UNKNOWN'))
    
    hist_map = {n:i for i,c in enumerate(nx.connected_components(G)) for n in c}
    df_test['Cluster'] = df_test['Code'].map(lambda x: f"H-{hist_map[x]}" if x in hist_map else "UNK")
    
    if STRICT_ZONE_MODE:
        new_clusters = []
        for idx, row in df_test.iterrows():
            if row['Cluster'] != 'UNK' and row['Region'] != 'UNKNOWN':
                new_clusters.append(f"{row['Cluster']}-{row['Region']}")
            else:
                new_clusters.append(row['Cluster'])
        df_test['Cluster'] = new_clusters
    
    mask_unk = df_test['Cluster']=="UNK"
    if mask_unk.any():
        mask_geo = (df_test['Lat']!=0) & mask_unk
        if mask_geo.any():
            coords = np.radians(df_test.loc[mask_geo, ['Lat','Lon']].values)
            db = DBSCAN(eps=MAX_KM_CLUSTER/6371.0, min_samples=1).fit(coords)
            df_test.loc[mask_geo, 'Cluster'] = [f"G-{x}" if x!=-1 else "NOISE" for x in db.labels_]
        
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏û‡∏¥‡∏Å‡∏±‡∏î ‡πÉ‡∏´‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ï‡∏≤‡∏° prefix ‡∏Ç‡∏≠‡∏á Code
        mask_no_geo = (df_test['Lat']==0) & mask_unk
        if mask_no_geo.any():
            def get_code_prefix(code):
                # ‡∏î‡∏∂‡∏á prefix ‡∏à‡∏≤‡∏Å‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô ZS, N, M, P)
                code_str = str(code)
                if len(code_str) >= 2:
                    # ‡∏ñ‡πâ‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ 2-3 ‡∏ï‡∏±‡∏ß
                    prefix = ''.join([c for c in code_str[:3] if c.isalpha()])
                    return f"PREFIX-{prefix}" if prefix else f"CODE-{code_str[:2]}"
                return f"SINGLE-{code_str}"
            
            df_test.loc[mask_no_geo, 'Cluster'] = df_test.loc[mask_no_geo, 'Code'].apply(get_code_prefix)
    
    mask_fin = df_test['Cluster'].isin(["UNK","NOISE"])
    if mask_fin.any():
        df_test.loc[mask_fin, 'Cluster'] = df_test.loc[mask_fin, 'Code'].map(
            lambda x: f"Z-{region_map.get(x, 'NEW')}" if x in region_map else f"NEW-{x}"
        )
    
    final_rows = []
    trip_cnt = 1
    
    for cid, group in df_test.groupby('Cluster'):
        pool = []
        for code, sub in group.groupby('Code'):
            pool.append({
                'Code': code, 'Name': sub.iloc[0]['Name'],
                'Wgt': sub['Wgt'].sum(), 'Cube': sub['Cube'].sum(),
                'Lat': sub.iloc[0]['Lat'], 'Lon': sub.iloc[0]['Lon']
            })
            
        while pool:
            pool.sort(key=lambda x: x['Cube'], reverse=True)
            current_truck = []
            seed = pool.pop(0)
            current_truck.append(seed)
            
            curr_w = seed['Wgt']
            curr_c = seed['Cube']
            last_lat = seed['Lat']
            last_lon = seed['Lon']
            last_name = seed['Name']
            drops = 1
            max_req = constraints.get(seed['Code'], 1)
            
            while True:
                best_idx = -1
                best_score = float('inf')
                best_is_same_name = False
                
                for i, cand in enumerate(pool):
                    if STRICT_ZONE_MODE:
                        if last_lat != 0 and cand['Lat'] != 0:
                            zone_dist = haversine(last_lat, last_lon, cand['Lat'], cand['Lon'])
                            if zone_dist > MAX_ZONE_DISTANCE:
                                continue
                        
                        if not is_same_zone(seed['Code'], cand['Code'], region_map, geo):
                            continue
                    
                    new_w = curr_w + cand['Wgt']
                    new_c = curr_c + cand['Cube']
                    
                    if new_w > 5800: continue
                    if new_c > 22.0 * BUFFER: continue
                    
                    is_same_name = is_similar_name(last_name, cand['Name'])
                    dist = haversine(last_lat, last_lon, cand['Lat'], cand['Lon']) if last_lat!=0 and cand['Lat']!=0 else 999
                    is_nearby = (dist <= NEARBY_RADIUS)
                    
                    if drops >= TARGET_DROPS:
                        if drops >= MAX_DROPS_FLEX: 
                            continue
                        if not (is_same_name or is_nearby): 
                            continue
                    
                    score = dist
                    if is_same_name:
                        score -= 1000
                    
                    is_better = (score < best_score)
                    if is_same_name and not best_is_same_name:
                        is_better = True
                    elif best_is_same_name and not is_same_name:
                        is_better = False
                    
                    if is_better:
                        best_score = score
                        best_idx = i
                        best_is_same_name = is_same_name
                        
                if best_idx != -1:
                    sel = pool.pop(best_idx)
                    current_truck.append(sel)
                    
                    curr_w += sel['Wgt']
                    curr_c += sel['Cube']
                    drops += 1
                    
                    if sel['Lat']!=0: 
                        last_lat = sel['Lat']; last_lon = sel['Lon']
                    last_name = sel['Name']
                    max_req = max(max_req, constraints.get(sel['Code'], 1))
                else:
                    break
            
            v_type = select_truck(curr_w, curr_c, max_req)
            tid = f"AI-{trip_cnt:03d}"
            
            for item in current_truck:
                final_rows.append({
                    'Booking No': tid, '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏£‡∏ñ': v_type,
                    '‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤': item['Code'], '‡∏™‡∏≤‡∏Ç‡∏≤': item['Name'],
                    'TOTALWGT': item['Wgt'], 'TOTALCUBE': item['Cube'],
                    'Remark': f"Drops:{drops}", 'Lat': item['Lat'], 'Lon': item['Lon']
                })
            trip_cnt += 1
            
    return pd.DataFrame(final_rows)

def export_styled_excel(df, filename):
    try:
        import xlsxwriter
        writer = pd.ExcelWriter(filename, engine='xlsxwriter')
        df.to_excel(writer, index=False, sheet_name='Plan')
        wb = writer.book; ws = writer.sheets['Plan']
        fmt_h = wb.add_format({'bold': True, 'bg_color': '#4472C4', 'font_color': 'white', 'border': 1})
        fmt_1 = wb.add_format({'bg_color': '#FFFFFF', 'border': 1})
        fmt_2 = wb.add_format({'bg_color': '#D9D9D9', 'border': 1})
        for c, val in enumerate(df.columns): ws.write(0, c, val, fmt_h)
        curr = None; toggle = False
        for r, row in df.iterrows():
            if row['Booking No'] != curr: toggle = not toggle; curr = row['Booking No']
            fmt = fmt_1 if toggle else fmt_2
            for c, val in enumerate(row): ws.write(r+1, c, val, fmt)
        writer.close()
    except:
        df.to_excel(filename, index=False)

# ==========================================
# 5. STREAMLIT UI
# ==========================================
def main():
    st.set_page_config(page_title="AI Logistics Planner", page_icon="üöö", layout="wide")
    
    st.title("üöö AI Logistics Planner: Sticky Routing Edition")
    st.markdown("---")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.info("‚ú® **Sticky Routing**: ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô + ‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ô‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô")
    with col2:
        st.info("üì¶ **Drop Rules**: 1-10 ‚úì | 11-12 (‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô/‡πÉ‡∏Å‡∏•‡πâ‚â§5km) ‚úì | 13+ ‚úó")
    with col3:
        st.info("üåè **Zone Filter**: Geofence 100km + Province/Region Aware")
    
    st.markdown("---")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå DC
    dc_folder = os.path.join(os.getcwd(), 'DC')
    dc_files_found = []
    if os.path.exists(dc_folder):
        dc_files_found = glob.glob(os.path.join(dc_folder, '*.xlsx')) + glob.glob(os.path.join(dc_folder, '*.xls'))
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå DC
    if dc_files_found:
        with st.expander(f"üìÇ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô DC/ : {len(dc_files_found)} ‡πÑ‡∏ü‡∏•‡πå", expanded=True):
            for f in dc_files_found:
                st.text(f"‚úì {os.path.basename(f)}")
    else:
        st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå 'DC/' ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå Excel ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå")
    
    st.markdown("---")
    
    # File uploader - ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Test
    st.subheader("üéØ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏≠‡πÄ‡∏î‡∏≠‡∏£‡πå (Test)")
    
    # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡πÉ‡∏ô session state (‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå)
    if 'last_uploaded_info' not in st.session_state:
        st.session_state.last_uploaded_info = None
    if 'result_ready' not in st.session_state:
        st.session_state.result_ready = False
    
    test_file = st.file_uploader("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå Test ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô", type=['xlsx', 'xls'], key='test')
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î)
    if test_file is not None:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á signature ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠ + ‡∏Ç‡∏ô‡∏≤‡∏î + ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        current_file_info = f"{test_file.name}_{test_file.size}_{test_file.tell()}"
        
        # ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î (‡πÅ‡∏°‡πâ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏¥‡∏°)
        if not st.session_state.result_ready or st.session_state.last_uploaded_info != current_file_info:
            st.session_state.last_uploaded_info = current_file_info
            st.session_state.result_ready = False
            st.cache_data.clear()
            st.success(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: {test_file.name}")
    elif test_file is None:
        # ‡∏ñ‡πâ‡∏≤‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏≠‡∏Å ‡πÉ‡∏´‡πâ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå session
        if st.session_state.last_uploaded_info is not None:
            st.session_state.last_uploaded_info = None
            st.session_state.result_ready = False
            st.cache_data.clear()
    
    st.markdown("---")
    
    if st.button("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô", type="primary", use_container_width=True):
        if not test_file:
            st.error("‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Test")
            return
        
        if not dc_files_found:
            st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå DC/ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå DC ‡πÅ‡∏•‡∏∞‡∏ß‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏ô‡∏±‡πâ‡∏ô")
            return
        
        with st.spinner("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•..."):
            # Load training data ‡∏à‡∏≤‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå DC
            tr_dfs = []
            
            st.info(f"üìÇ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå DC/ ({len(dc_files_found)} ‡πÑ‡∏ü‡∏•‡πå)")
            
            for dc_file_path in dc_files_found:
                try:
                    with open(dc_file_path, 'rb') as f:
                        file_content = f.read()
                        train_df = process_dataframe(load_excel(file_content))
                        if train_df is not None:
                            tr_dfs.append(train_df)
                            st.success(f"‚úÖ {os.path.basename(dc_file_path)}: {len(train_df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
                        else:
                            st.warning(f"‚ö†Ô∏è {os.path.basename(dc_file_path)}: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ")
                except Exception as e:
                    st.error(f"‚ùå {os.path.basename(dc_file_path)}: {str(e)}")
            
            if not tr_dfs:
                st.error("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏î‡πÜ ‡∏à‡∏≤‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå DC ‡πÑ‡∏î‡πâ")
                return
            
            st.info(f"üìö ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ó‡∏£‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(tr_dfs)} ‡πÑ‡∏ü‡∏•‡πå")
            
            # Train AI
            G, const, regions, learning_stats = train_ai(tr_dfs)
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
            st.success(f"üß† ‡πÄ‡∏ó‡∏£‡∏ô AI ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
            
            with st.expander("üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥", expanded=True):
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("üöö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏£‡∏¥‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ", f"{learning_stats['total_trips']}")
                with col2:
                    st.metric("üè™ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", f"{learning_stats['total_branches']}")
                with col3:
                    st.metric("üìç ‡∏à‡∏∏‡∏î‡∏™‡πà‡∏á‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢/‡∏ó‡∏£‡∏¥‡∏õ", f"{learning_stats['avg_drops']:.1f}")
                with col4:
                    st.metric("üó∫Ô∏è ‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢", f"{learning_stats['avg_distance']:.0f} km")
                
                st.markdown("---")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.write("**üåè ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ:**")
                    for region, count in sorted(learning_stats['region_distribution'].items(), key=lambda x: x[1], reverse=True):
                        if region != 'UNKNOWN':
                            st.write(f"- {region}: {count} ‡∏ó‡∏£‡∏¥‡∏õ")
                
                with col2:
                    st.write("**üöõ ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏£‡∏ñ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó:**")
                    for veh, count in sorted(learning_stats['vehicle_usage'].items(), key=lambda x: x[1], reverse=True):
                        st.write(f"- {veh}: {count} ‡∏ó‡∏£‡∏¥‡∏õ")
                
                st.info(f"üí° ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏ñ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥")
            
            # Load geo - ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å training files
            geo = {}
            for df in tr_dfs:
                if df is not None:
                    temp_geo = process_geo(df)
                    geo.update(temp_geo)
            
            if geo:
                st.success(f"üìç ‡∏î‡∏∂‡∏á‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ó‡∏£‡∏ô: {len(geo)} ‡∏™‡∏≤‡∏Ç‡∏≤")
            else:
                st.info("üìç ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏¥‡∏Å‡∏±‡∏î‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ó‡∏£‡∏ô")
            
            # Process test data
            test_content = test_file.read()
            df_test = process_dataframe(load_excel(test_content))
            if df_test is None:
                st.error("‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Test")
                return
            
            st.info(f"üì¶ ‡∏≠‡∏≠‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(df_test)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á: {df_test['Code'].nunique()} ‡∏™‡∏≤‡∏Ç‡∏≤")
            
            # ‡∏î‡∏∂‡∏á‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏à‡∏≤‡∏Å‡∏ä‡∏µ‡∏ï Location ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå Test (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            test_file.seek(0)  # reset file pointer
            df_location = load_excel(test_file.read(), sheet_name='Location')
            if df_location is not None:
                df_location_processed = process_dataframe(df_location)
                if df_location_processed is not None:
                    location_geo = process_geo(df_location_processed)
                    if location_geo:
                        geo.update(location_geo)
                        st.success(f"üìç ‡∏î‡∏∂‡∏á‡∏û‡∏¥‡∏Å‡∏±‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏ä‡∏µ‡∏ï Location: {len(location_geo)} ‡∏™‡∏≤‡∏Ç‡∏≤")
            
            st.info(f"üìç ‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(geo)} ‡∏™‡∏≤‡∏Ç‡∏≤")
            
            # Run prediction
            st.info("üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á...")
            res = run_prediction(df_test, G, geo, const, regions)
            
            # Post-processing: ‡∏£‡∏ß‡∏°‡∏ó‡∏£‡∏¥‡∏õ‡πÄ‡∏•‡πá‡∏Å‡πÜ
            st.info("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏ó‡∏£‡∏¥‡∏õ‡πÄ‡∏•‡πá‡∏Å‡πÜ ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ...")
            res = merge_small_trips(res, geo, regions)
            
            res = res.sort_values(by=['Booking No', 'Lat'])
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏•‡πâ‡∏ß
            st.session_state.result_ready = True
            
            # Display results
            total_trips = res['Booking No'].nunique()
            trip_summary = res.groupby('Booking No').agg({
                '‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤': 'count',
                'TOTALWGT': 'sum',
                'TOTALCUBE': 'sum'
            }).rename(columns={'‡∏£‡∏´‡∏±‡∏™‡∏™‡∏≤‡∏Ç‡∏≤': 'Drops'})
            
            st.markdown("---")
            st.success("### ‚úÖ ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üöö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß", f"{total_trips} ‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß")
            with col2:
                st.metric("üìç ‡∏à‡∏∏‡∏î‡∏™‡πà‡∏á‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢", f"{trip_summary['Drops'].mean():.1f} ‡∏à‡∏∏‡∏î/‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß")
            with col3:
                st.metric("‚öñÔ∏è ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢", f"{trip_summary['TOTALWGT'].mean():.0f} kg/‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß")
            with col4:
                st.metric("üì¶ ‡∏Ñ‡∏¥‡∏ß‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢", f"{trip_summary['TOTALCUBE'].mean():.2f} cbm/‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß")
            
            # Display dataframe
            st.subheader("üìã ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå")
            st.dataframe(res, use_container_width=True, height=400)
            
            # Export
            output_filename = 'AI_Sticky_Routing_Plan.xlsx'
            export_styled_excel(res, output_filename)
            
            with open(output_filename, 'rb') as f:
                st.download_button(
                    label="üíæ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel",
                    data=f,
                    file_name=output_filename,
                    mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                    use_container_width=True
                )
            
            st.balloons()

if __name__ == "__main__":
    main()
